{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":952451,"sourceType":"datasetVersion","datasetId":517218},{"sourceId":2255937,"sourceType":"datasetVersion","datasetId":1357458},{"sourceId":8857973,"sourceType":"datasetVersion","datasetId":5223027},{"sourceId":14083871,"sourceType":"datasetVersion","datasetId":8966663}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# @title 1Ô∏è‚É£ SETUP: Clone Project & Submodules\nimport os\nimport shutil\nimport subprocess\nimport torch\n\n# --- C·∫§U H√åNH ---\nREPO_URL = \"https://github.com/rishubhpar/Depth-Aware-Editing.git\"\nTEMP_DIR = \"/kaggle/temp/Depth-Aware-Editing\"\nWORK_LINK = \"/kaggle/working/Depth-Aware-Editing\"\n\nprint(\"üöÄ B·∫ÆT ƒê·∫¶U KH·ªûI T·∫†O (Mode: Stable Diffusion 1.5)...\")\n\n# 1. Ki·ªÉm tra GPU\nif not torch.cuda.is_available():\n    print(\"‚ö†Ô∏è C·∫¢NH B√ÅO: Ch∆∞a b·∫≠t GPU! V√†o Session Options -> Accelerator -> GPU T4 x2.\")\nelse:\n    print(f\"‚úÖ GPU OK: {torch.cuda.get_device_name(0)}\")\n\ndef run_cmd(cmd):\n    subprocess.run(cmd, shell=True, stdout=subprocess.DEVNULL)\n\n# 2. D·ªçn d·∫πp & Clone\nif os.path.exists(\"/kaggle/working\"): os.chdir(\"/kaggle/working\")\nif os.path.exists(WORK_LINK):\n    if os.path.islink(WORK_LINK): os.unlink(WORK_LINK)\n    else: shutil.rmtree(WORK_LINK)\nif os.path.exists(TEMP_DIR): shutil.rmtree(TEMP_DIR)\n\nprint(\"‚¨áÔ∏è ƒêang clone Repository ch√≠nh...\")\nrun_cmd(f\"git clone {REPO_URL} {TEMP_DIR}\")\nos.symlink(TEMP_DIR, WORK_LINK)\nos.chdir(TEMP_DIR)\n\n# 3. Clone Submodules Th·ªß c√¥ng\nprint(\"üèóÔ∏è ƒêang clone c√°c Module con (SAM & GroundingDINO)...\")\nrun_cmd(\"git clone https://github.com/IDEA-Research/Grounded-Segment-Anything.git src/grounded_sam\")\nif os.path.exists(\"src/grounded_sam/GroundingDINO\"): shutil.rmtree(\"src/grounded_sam/GroundingDINO\")\nrun_cmd(\"git clone https://github.com/IDEA-Research/GroundingDINO.git src/grounded_sam/GroundingDINO\")\n\nprint(\"‚úÖ B∆Ø·ªöC 1 HO√ÄN T·∫§T.\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title 2Ô∏è‚É£ INSTALL: C√†i ƒë·∫∑t Th∆∞ vi·ªán (FORCE OVERRIDE VERSION)\nimport os\nimport subprocess\nimport sys\n\n# ƒê·∫£m b·∫£o l√†m vi·ªác t·∫°i th∆∞ m·ª•c temp\nos.chdir(\"/kaggle/temp/Depth-Aware-Editing\")\n\nprint(\"üì¶ ƒêang c√†i ƒë·∫∑t th∆∞ vi·ªán (Ch·∫ø ƒë·ªô C∆∞·ª°ng b·ª©c Version)...\")\nprint(\"‚ö†Ô∏è Code n√†y s·∫Ω g·ª° v√† c√†i l·∫°i nhi·ªÅu l·∫ßn ƒë·ªÉ √©p ƒë√∫ng version. H√£y ki√™n nh·∫´n!\\n\")\n\ndef run_cmd(cmd, desc=\"\"):\n    if desc:\n        print(f\"    ‚¨áÔ∏è {desc}...\")\n    subprocess.run(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n\n# ============================================================\n# B∆Ø·ªöC 1: D·ªåN D·∫∏P S·∫†CH S·∫º\n# ============================================================\nprint(\"üßπ B∆∞·ªõc 1/6: G·ª° b·ªè c√°c th∆∞ vi·ªán g√¢y xung ƒë·ªôt...\")\nuninstall_pkgs = [\n    \"numpy\", \"scipy\", \"scikit-learn\", \"matplotlib\", \"torchmetrics\",\n    \"torch\", \"torchvision\", \"torchaudio\", \"xformers\", \n    \"pytorch-lightning\", \"lightning\",\n    \"transformers\", \"diffusers\", \"accelerate\", \"huggingface_hub\", \"peft\",\n    \"gradio\", \"opencv-python\", \"opencv-python-headless\", \"albumentations\",\n    \"segment-anything\", \"groundingdino\", \n    \"tensorflow\", \"keras\", \"tensorboard\", \n    \"jax\", \"jaxlib\"\n]\nrun_cmd(f\"pip uninstall -y {' '.join(uninstall_pkgs)}\")\n\n# ============================================================\n# B∆Ø·ªöC 2: C√ÄI PYTORCH & CUDA\n# ============================================================\nprint(\"üî• B∆∞·ªõc 2/6: C√†i ƒë·∫∑t PyTorch 2.1...\")\nrun_cmd(\n    \"pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 \"\n    \"--index-url https://download.pytorch.org/whl/cu121\"\n)\nrun_cmd(\n    \"pip install xformers==0.0.23 post1 --index-url https://download.pytorch.org/whl/cu121\"\n)\n\n# ============================================================\n# B∆Ø·ªöC 3: C√ÄI C√ÅC TH∆Ø VI·ªÜN N·ªÄN T·∫¢NG (Transformers, Accelerate...)\n# ============================================================\nprint(\"ü§ñ B∆∞·ªõc 3/6: C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán n·ªÅn (Cho ph√©p c√†i Hub m·ªõi t·∫°m th·ªùi)...\")\n# ·ªû b∆∞·ªõc n√†y, ta c·ª© ƒë·ªÉ n√≥ c√†i b·∫£n m·ªõi nh·∫•t n·∫øu n√≥ mu·ªën, ta s·∫Ω s·ª≠a sau\nai_libs = [\n    \"opencv-python-headless\", \"albumentations\", \"imageio\", \"moviepy\",\n    \"transformers==4.39.3\", \"accelerate==0.28.0\",\n    \"safetensors\", \"omegaconf\", \"einops\", \"timm\",\n    \"addict\", \"yapf\", \"pyyaml\", \"gradio==3.50.2\", \"protobuf==3.20.3\",\n    \"scikit-learn\", \"matplotlib\", \n    \"pytorch-lightning==2.0.9\", \"torchmetrics==1.2.1\"\n]\nrun_cmd(f\"pip install {' '.join(ai_libs)}\")\n\n# ============================================================\n# B∆Ø·ªöC 4: C√ÄI MODULES NGO√ÄI (SAM & DINO)\n# ============================================================\nprint(\"üéØ B∆∞·ªõc 4/6: C√†i ƒë·∫∑t SAM & GroundingDINO...\")\nrun_cmd(\"pip install git+https://github.com/facebookresearch/segment-anything.git\")\nrun_cmd(\"pip install git+https://github.com/IDEA-Research/GroundingDINO.git\")\n\n# ============================================================\n# B∆Ø·ªöC 5: C∆Ø·ª†NG B·ª®C C√ÄI ƒê·∫∂T C√ÅC VERSION QUAN TR·ªåNG (THE FIX)\n# ============================================================\nprint(\"üî® B∆∞·ªõc 5/6: C∆∞·ª°ng b·ª©c c√†i ƒë·∫∑t Diffusers, PEFT v√† Hub c≈©...\")\n\n# 1. G·ª° b·ªè b·∫£n Hub m·ªõi (0.36.0) m√† c√°c th∆∞ vi·ªán tr√™n v·ª´a l√©n c√†i v√†o\nrun_cmd(\"pip uninstall -y huggingface_hub diffusers peft\")\n\n# 2. C√†i ƒë·∫∑t l·∫°i v·ªõi c·ªù --no-deps (Kh√¥ng c√†i dependencies ph·ª• thu·ªôc ƒë·ªÉ tr√°nh update ng∆∞·ª£c l·∫°i)\n# ƒê√¢y l√† b∆∞·ªõc quan tr·ªçng nh·∫•t: √âp bu·ªôc d√πng ƒë√∫ng version n√†y.\ninstall_force = [\n    \"huggingface_hub<0.25.0\",  # Fix l·ªói cached_download\n    \"diffusers==0.27.2\",       # Fix l·ªói GLIGEN\n    \"peft==0.8.2\"              # Fix l·ªói t∆∞∆°ng th√≠ch\n]\nfor pkg in install_force:\n    print(f\"      üëâ √âp c√†i: {pkg}\")\n    run_cmd(f\"pip install '{pkg}' --force-reinstall --no-deps\")\n\n# ============================================================\n# B∆Ø·ªöC 6: CH·ªêT H·∫† VERSION NUMPY & SCIPY\n# ============================================================\nprint(\"üî® B∆∞·ªõc 6/7: √âp phi√™n b·∫£n NumPy 1.26.4...\")\nrun_cmd(\"pip uninstall -y jax jaxlib numpy scipy\")\nrun_cmd(\"pip install 'numpy==1.26.4' 'scipy==1.11.4' --force-reinstall\")\n\n# TH√äM: Fix numpy.dtype binary incompatibility\nprint(\"üîß B∆∞·ªõc 7/7: Recompile packages ph·ª• thu·ªôc numpy...\")\nrun_cmd(\"pip install --force-reinstall --no-cache-dir scikit-learn matplotlib pillow opencv-python-headless\")\n\n# ============================================================\n# KI·ªÇM TRA K·∫æT QU·∫¢\n# ============================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"üîç KI·ªÇM TRA PHI√äN B·∫¢N (FINAL CHECK):\")\nprint(\"=\"*60)\n\ncheck_cmds = [\n    (\"NumPy (Target: 1.26.4)\", \"import numpy; print(numpy.__version__)\"),\n    (\"Diffusers (Target: 0.27.2)\", \"import diffusers; print(diffusers.__version__)\"),\n    (\"HuggingFace Hub (Target: <0.25)\", \"import huggingface_hub; print(huggingface_hub.__version__)\"),\n    (\"PEFT (Target: 0.8.2)\", \"import peft; print(peft.__version__)\")\n]\n\nfor name, cmd in check_cmds:\n    result = subprocess.run([sys.executable, \"-c\", cmd], capture_output=True, text=True)\n    ver = result.stdout.strip() if result.returncode == 0 else \"ERROR\"\n    \n    # Logic ki·ªÉm tra m√†u\n    status = \"‚úÖ\"\n    if \"ERROR\" in ver: \n        status = \"‚ùå\"\n    elif name.startswith(\"HuggingFace\") and ver.startswith(\"0.3\"): # N·∫øu Hub l·∫°i nh·∫£y l√™n 0.3x\n        status = \"‚ùå (V·∫´n b·ªã ghi ƒë√®!)\"\n    \n    print(f\"    {status} {name}: {ver}\")\n\nprint(\"=\"*60)\nprint(\"‚úÖ QU√Å TR√åNH C√ÄI ƒê·∫∂T HO√ÄN T·∫§T!\")\n\nprint(\"\\n‚ö†Ô∏è  B∆Ø·ªöC QUY·∫æT ƒê·ªäNH: RESTART SESSION NGAY.\")\nprint(\"=\"*60)\n\nprint(\"    üëâ Ch·ªçn Menu: Run -> Restart Session\")\nprint(\"    üëâ Sau khi Restart: Ch·∫°y ti·∫øp Cell 3 (ƒë√£ s·ª≠a ·ªü tr√™n), r·ªìi Cell 4, 5.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title 3Ô∏è‚É£ PATCH: V√° l·ªói TO√ÄN DI·ªÜN (B·∫£n \"Direct Import\" - B·ªè qua torch.hub)\nimport os\nimport subprocess\nimport sys\nimport shutil\n\nWORK_DIR = \"/kaggle/temp/Depth-Aware-Editing\"\nif os.path.exists(WORK_DIR):\n    os.chdir(WORK_DIR)\nelse:\n    raise FileNotFoundError(f\"Kh√¥ng t√¨m th·∫•y {WORK_DIR}. Ch·∫°y l·∫°i Cell 1!\")\n\nprint(\"üöë ƒêang ch·∫°y b·∫£n v√° l·ªói (Chi·∫øn thu·∫≠t: Direct Import - Lo·∫°i b·ªè torch.hub)...\")\n\ndef run_cmd(cmd):\n    subprocess.run(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n\n# ============================================================\n# 1. C√ÄI TH∆Ø VI·ªÜN & SETUP DINOv2 (QUAN TR·ªåNG)\n# ============================================================\nprint(\"    üì• [1/10] Setup m√¥i tr∆∞·ªùng & DINOv2...\")\nrun_cmd(\"pip install open-clip-torch\")\n\n# Clone v√† copy DINOv2 ra root ƒë·ªÉ Python nh√¨n th·∫•y tr·ª±c ti·∫øp\ndinov2_repo = os.path.join(os.getcwd(), \"dinov2_repo\")\nif not os.path.exists(dinov2_repo):\n    subprocess.run(f\"git clone -q https://github.com/facebookresearch/dinov2.git {dinov2_repo}\", shell=True)\n\n# Copy folder 'dinov2' (package) ra th∆∞ m·ª•c g·ªëc\nsource_dinov2 = os.path.join(dinov2_repo, \"dinov2\")\ntarget_dinov2 = os.path.join(os.getcwd(), \"dinov2\")\n\nif os.path.exists(source_dinov2):\n    if os.path.exists(target_dinov2):\n        shutil.rmtree(target_dinov2)\n    shutil.copytree(source_dinov2, target_dinov2)\n    print(\"      ‚úÖ ƒê√£ ƒë·∫∑t DINOv2 v√†o v·ªã tr√≠ chu·∫©n.\")\n\n# ============================================================\n# 2. V√Å FILE modules.py (LO·∫†I B·ªé torch.hub.load)\n# ============================================================\nprint(\"    üîß [2/10] Ph·∫´u thu·∫≠t file modules.py (Bypass torch.hub)...\")\nmodules_file = \"ldm/modules/encoders/modules.py\"\n\nif os.path.exists(modules_file):\n    with open(modules_file, \"r\") as f:\n        code = f.read()\n\n    # ƒêo·∫°n code m·ªõi: Import tr·ª±c ti·∫øp thay v√¨ d√πng torch.hub\n    header_patch = \"\"\"\nimport sys, os\n# ƒê·∫£m b·∫£o Python t√¨m th·∫•y dinov2 ·ªü th∆∞ m·ª•c hi·ªán t·∫°i\nsys.path.append(os.getcwd()) \ntry:\n    from dinov2.hub.backbones import dinov2_vitg14\nexcept ImportError:\n    print(\"‚ö†Ô∏è Warning: Import dinov2 failed initially, retrying...\")\n    from dinov2_repo.dinov2.hub.backbones import dinov2_vitg14\n\"\"\"\n    if \"from dinov2.hub.backbones\" not in code:\n        code = header_patch + \"\\n\" + code\n\n    # Thay th·∫ø chu·ªói string g·ªçi h√†m\n    target_str_1 = \"torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\"\n    target_str_2 = \"torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14', source='local', pretrained=False)\"\n    target_str_3 = f\"torch.hub.load('{dinov2_repo}', 'dinov2_vitg14', source='local', pretrained=False)\"\n    \n    replacement = \"dinov2_vitg14(pretrained=False)\"\n    \n    code = code.replace(target_str_1, replacement)\n    code = code.replace(target_str_2, replacement)\n    code = code.replace(target_str_3, replacement)\n    \n    # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p code ƒë√£ b·ªã s·ª≠a b·ªüi cell tr∆∞·ªõc (lambda) ho·∫∑c bi·∫øn th·ªÉ kh√°c\n    if \"source='local'\" in code and \"dinov2_vitg14\" in code:\n        lines = code.split('\\n')\n        new_lines = []\n        for line in lines:\n            if \"torch.hub.load\" in line and \"dinov2\" in line:\n                indent = line[:len(line) - len(line.lstrip())]\n                # Thay th·∫ø tr·ª±c ti·∫øp d√≤ng g·ªçi l·ªánh c≈© b·∫±ng l·ªánh m·ªõi\n                new_line = line.replace(\"torch.hub.load('/kaggle/temp/Depth-Aware-Editing/dinov2_repo', 'dinov2_vitg14', source='local', pretrained=False)\", replacement)\n                \n                # Fix cho c√°c bi·∫øn th·ªÉ kh√°c n·∫øu replace tr√™n kh√¥ng b·∫Øt ƒë∆∞·ª£c\n                if \"torch.hub.load\" in new_line: \n                     new_line = indent + f\"dinov2 = {replacement}\" # <--- ƒê√É S·ª¨A L·ªñI ·ªû ƒê√ÇY (th√™m d·∫•u ngo·∫∑c k√©p)\n                \n                new_lines.append(new_line)\n            else:\n                new_lines.append(line)\n        code = '\\n'.join(new_lines)\n\n    # X√≥a s·∫°ch c√°c d√≤ng import hubconf th·ª´a th√£i\n    code = code.replace(\"import hubconf\", \"# import hubconf\")\n    \n    with open(modules_file, \"w\") as f:\n        f.write(code)\n    print(\"      ‚úÖ ƒê√£ thay th·∫ø torch.hub.load b·∫±ng Direct Import.\")\n\n# ============================================================\n# 3. FIX C√ÅC L·ªñI KH√ÅC (NH∆Ø C≈®)\n# ============================================================\nprint(\"    üîÑ [3/10] Fix datasets, PositionNet, CaptionProjection...\")\nif os.path.exists(\"datasets\") and os.path.isdir(\"datasets\"):\n    shutil.move(\"datasets\", \"repo_datasets\")\nrun_cmd(\"find . -name '*.py' -print0 | xargs -0 sed -i 's/from datasets/from repo_datasets/g'\")\nrun_cmd(\"find . -name '*.py' -print0 | xargs -0 sed -i 's/import datasets/import repo_datasets/g'\")\n\n# Fix UNet\ntarget_unet = \"src/featglac/model/unet_2d_condition.py\"\nif os.path.exists(target_unet):\n    with open(target_unet, \"r\") as f: code = f.read()\n    if \"PositionNet,\" in code: code = code.replace(\"PositionNet,\", \"\")\n    dummy_pos = \"import torch.nn as nn\\nclass PositionNet(nn.Module):\\n    def __init__(self, *args, **kwargs): super().__init__()\\n    def forward(self, x, *args, **kwargs): return x\"\n    if \"class PositionNet\" not in code:\n        code = code.replace(\"from diffusers.models.embeddings import (\", dummy_pos + \"\\nfrom diffusers.models.embeddings import (\")\n    with open(target_unet, \"w\") as f: f.write(code)\n\n# Fix Transformer\ntarget_trans = \"src/featglac/model/transformer_2d.py\"\nif os.path.exists(target_trans):\n    with open(target_trans, \"r\") as f: code = f.read()\n    caption_proj = \"import torch.nn as nn\\nclass CaptionProjection(nn.Module):\\n    def __init__(self, i, h): super().__init__(); self.l1=nn.Linear(i,h); self.a=nn.GELU(); self.l2=nn.Linear(h,h)\\n    def forward(self, x): return self.l2(self.a(self.l1(x)))\"\n    if \"class CaptionProjection\" not in code:\n        code = code.replace(\"from diffusers.models.embeddings import CaptionProjection, PatchEmbed\", caption_proj + \"\\nfrom diffusers.models.embeddings import PatchEmbed\")\n        code = code.replace(\"CaptionProjection,\", \"\")\n        if \"class CaptionProjection\" not in code: code = caption_proj + \"\\n\" + code\n    with open(target_trans, \"w\") as f: f.write(code)\n\n# Fix Config & PL\nrun_cmd(\"find . -name '*.py' -print0 | xargs -0 sed -i 's|stabilityai/stable-diffusion-2-1-base|runwayml/stable-diffusion-v1-5|g'\")\nrun_cmd(\"find . -name '*.yaml' -print0 | xargs -0 sed -i 's|stabilityai/stable-diffusion-2-1-base|runwayml/stable-diffusion-v1-5|g'\")\nrun_cmd(\"find . -name '*.py' -print0 | xargs -0 sed -i 's/pytorch_lightning.utilities.distributed/pytorch_lightning.utilities.rank_zero/g'\")\n\n# ============================================================\n# 4. VI·∫æT L·∫†I FILE grounded_sam_demo1.py\n# ============================================================\nprint(\"    üìù [4/10] Vi·∫øt file grounded_sam_demo1.py...\")\ndemo_code = r'''\nimport sys, os\nsys.path.append(os.path.join(os.getcwd(), 'src', 'grounded_sam')); sys.path.append(os.path.join(os.getcwd(), 'src', 'grounded_sam', 'GroundingDINO'))\nimport argparse, json, numpy as np, torch, cv2\nfrom PIL import Image; import matplotlib.pyplot as plt\nimport GroundingDINO.groundingdino.datasets.transforms as T\nfrom GroundingDINO.groundingdino.models import build_model; from GroundingDINO.groundingdino.util.slconfig import SLConfig\nfrom GroundingDINO.groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\ntry: from segment_anything import sam_model_registry, sam_hq_model_registry, SamPredictor\nexcept: from segment_anything import sam_model_registry, SamPredictor; sam_hq_model_registry = None\ndef load_image(p): i = Image.open(p).convert(\"RGB\"); t = T.Compose([T.RandomResize([800], max_size=1333), T.ToTensor(), T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]); return i, t(i, None)[0]\ndef load_model(c, ck, b, d): a=SLConfig.fromfile(c); a.device=d; a.bert_base_uncased_path=b; m=build_model(a); m.load_state_dict(clean_state_dict(torch.load(ck, map_location=\"cpu\")[\"model\"]), strict=False); m.eval(); return m\ndef get_grounding_output(m, i, c, b, t, w=True, d=\"cpu\"):\n    c=c.lower().strip(); c=c+\".\" if not c.endswith(\".\") else c; m=m.to(d); i=i.to(d)\n    with torch.no_grad(): o=m(i[None], captions=[c])\n    l=o[\"pred_logits\"].cpu().sigmoid()[0]; bx=o[\"pred_boxes\"].cpu()[0]; f=l.max(dim=1)[0]>b; lf=l[f]; bf=bx[f]; tok=m.tokenizer(c); p=[]\n    for lg in lf: ph=get_phrases_from_posmap(lg>t, tok, m.tokenizer); p.append(ph+f\"({str(lg.max().item())[:4]})\" if w else ph)\n    return bf, p\ndef main(a):\n    os.makedirs(a.output_dir, exist_ok=True); ip, i=load_image(a.input_image); m=load_model(a.config, a.grounded_checkpoint, a.bert_base_uncased_path, a.device)\n    bf, pp=get_grounding_output(m, i, a.text_prompt, a.box_threshold, a.text_threshold, device=a.device)\n    s=(sam_hq_model_registry if a.use_sam_hq else sam_model_registry)[a.sam_version](checkpoint=a.sam_hq_checkpoint if a.use_sam_hq else a.sam_checkpoint).to(a.device)\n    pr=SamPredictor(s); ic=cv2.cvtColor(cv2.imread(a.input_image), cv2.COLOR_BGR2RGB); pr.set_image(ic)\n    H,W=ip.size[1],ip.size[0]\n    for k in range(bf.size(0)): bf[k]=bf[k]*torch.Tensor([W,H,W,H]); bf[k][:2]-=bf[k][2:]/2; bf[k][2:]+=bf[k][:2]\n    tb=pr.transform.apply_boxes_torch(bf, ic.shape[:2]).to(a.device); ma,_,_=pr.predict_torch(None, None, boxes=tb, multimask_output=False)\n    plt.figure(figsize=(10,10)); plt.imshow(ic)\n    for mk in ma: c=np.concatenate([np.random.random(3), np.array([0.6])]); h,w=mk.shape[-2:]; plt.gca().imshow(mk.cpu().numpy().reshape(h,w,1)*c.reshape(1,1,-1))\n    for bx,lb in zip(bf,pp): x,y,w,h=bx[0],bx[1],bx[2]-bx[0],bx[3]-bx[1]; plt.gca().add_patch(plt.Rectangle((x,y),w,h,edgecolor='green',facecolor='none',lw=2)); plt.gca().text(x,y,lb)\n    plt.axis('off'); plt.savefig(os.path.join(a.output_dir, \"grounded_sam_output.jpg\"), bbox_inches=\"tight\", pad_inches=0); plt.close()\nif __name__==\"__main__\": pass \n'''\nwith open(\"src/grounded_sam/grounded_sam_demo1.py\", \"w\") as f: f.write(demo_code)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ ƒê√É V√Å XONG (Direct Import Strategy)!\")\nprint(\"üëâ ƒê√¢y l√† bi·ªán ph√°p m·∫°nh nh·∫•t. H√£y ch·∫°y l·∫°i Cell 4.\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T03:12:52.290317Z","iopub.execute_input":"2025-12-25T03:12:52.290825Z","iopub.status.idle":"2025-12-25T03:12:57.285150Z","shell.execute_reply.started":"2025-12-25T03:12:52.290795Z","shell.execute_reply":"2025-12-25T03:12:57.284395Z"}},"outputs":[{"name":"stdout","text":"üöë ƒêang ch·∫°y b·∫£n v√° l·ªói (Chi·∫øn thu·∫≠t: Direct Import - Lo·∫°i b·ªè torch.hub)...\n    üì• [1/10] Setup m√¥i tr∆∞·ªùng & DINOv2...\n      ‚úÖ ƒê√£ ƒë·∫∑t DINOv2 v√†o v·ªã tr√≠ chu·∫©n.\n    üîß [2/10] Ph·∫´u thu·∫≠t file modules.py (Bypass torch.hub)...\n      ‚úÖ ƒê√£ thay th·∫ø torch.hub.load b·∫±ng Direct Import.\n    üîÑ [3/10] Fix datasets, PositionNet, CaptionProjection...\n    üìù [4/10] Vi·∫øt file grounded_sam_demo1.py...\n\n============================================================\n‚úÖ ƒê√É V√Å XONG (Direct Import Strategy)!\nüëâ ƒê√¢y l√† bi·ªán ph√°p m·∫°nh nh·∫•t. H√£y ch·∫°y l·∫°i Cell 4.\n============================================================\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# @title 4Ô∏è‚É£ WEIGHTS: Chu·∫©n b·ªã Model\nimport os\n\nos.chdir(\"/kaggle/temp/Depth-Aware-Editing\")\nweights_dir = \"weights\"\nos.makedirs(weights_dir, exist_ok=True)\n\nprint(\"üèãÔ∏è‚Äç‚ôÇÔ∏è ƒêang chu·∫©n b·ªã Model...\")\n\nmodels = {\n    \"depth_anything_metric_depth_indoor.pt\": (\"depth_anything_metric_depth_indoor\", \"https://huggingface.co/LiheYoung/Depth-Anything/resolve/main/checkpoints_metric_depth/depth_anything_metric_depth_indoor.pt\"),\n    \"depth_anything_vitl14.pth\": (\"depth_anything_vitl14\", \"https://huggingface.co/spaces/LiheYoung/Depth-Anything/resolve/main/checkpoints/depth_anything_vitl14.pth\"),\n    \"dinov2_vitg14_pretrain.pth\": (\"dinov2_vitg14\", \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_pretrain.pth\"),\n    \"epoch=1-step=8687.ckpt\": (\"8687\", \"https://huggingface.co/spaces/xichenhku/AnyDoor/resolve/main/epoch%3D1-step%3D8687.ckpt\"),\n    \"sd-v1-5-inpainting.ckpt\": (\"inpainting\", \"https://huggingface.co/runwayml/stable-diffusion-inpainting/resolve/main/sd-v1-5-inpainting.ckpt\")\n}\n\nfor target, (key, url) in models.items():\n    dest = os.path.join(weights_dir, target)\n    found = False\n    for root, _, files in os.walk(\"/kaggle/input\"):\n        for f in files:\n            if key in f:\n                if os.path.exists(dest) or os.path.islink(dest): os.remove(dest)\n                os.symlink(os.path.join(root, f), dest)\n                print(f\"üîó Linked: {target}\")\n                found = True\n                break\n        if found: break\n    \n    if not found:\n        if not os.path.exists(dest) or os.path.getsize(dest) < 1024*1024:\n            print(f\"‚¨áÔ∏è Downloading: {target}...\")\n            os.system(f\"wget -q -O '{dest}' '{url}'\")\n        else:\n            print(f\"‚úÖ ƒê√£ c√≥ s·∫µn: {target}\")\n\nprint(\"‚úÖ B∆Ø·ªöC 4 HO√ÄN T·∫§T.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T03:13:02.457388Z","iopub.execute_input":"2025-12-25T03:13:02.458114Z","iopub.status.idle":"2025-12-25T03:28:08.739453Z","shell.execute_reply.started":"2025-12-25T03:13:02.458086Z","shell.execute_reply":"2025-12-25T03:28:08.738674Z"}},"outputs":[{"name":"stdout","text":"üèãÔ∏è‚Äç‚ôÇÔ∏è ƒêang chu·∫©n b·ªã Model...\nüîó Linked: depth_anything_metric_depth_indoor.pt\nüîó Linked: depth_anything_vitl14.pth\nüîó Linked: dinov2_vitg14_pretrain.pth\n‚¨áÔ∏è Downloading: epoch=1-step=8687.ckpt...\n‚¨áÔ∏è Downloading: sd-v1-5-inpainting.ckpt...\n‚úÖ B∆Ø·ªöC 4 HO√ÄN T·∫§T.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# @title 5Ô∏è‚É£ SETUP MODULE 1: Segmentation (FIX: Size Mismatch & NumPy Types)\nimport os\nimport sys\n\n# 1. ƒê·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n\nif os.path.exists(\"/kaggle/temp/Depth-Aware-Editing\"):\n    os.chdir(\"/kaggle/temp/Depth-Aware-Editing\")\n\nprint(\"üöÄ ƒêang thi·∫øt l·∫≠p Module 1 & V√° l·ªói k√≠ch th∆∞·ªõc ·∫£nh...\")\n\n# ------------------------------------------------------------------------------\n# B∆Ø·ªöC 1: T·∫†O HUBCONF\n# ------------------------------------------------------------------------------\nhubconf_code = \"\"\"\nimport torch\nimport os\ndef dinov2_vitg14(pretrained=False, **kwargs):\n    local_repo = os.path.abspath(\"dinov2_repo\")\n    if os.path.exists(local_repo):\n        try: return torch.hub.load(local_repo, 'dinov2_vitg14', source='local', pretrained=False, **kwargs)\n        except: pass\n    return torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14', **kwargs)\n\"\"\"\nwith open(\"hubconf.py\", \"w\") as f: f.write(hubconf_code)\n\n# ------------------------------------------------------------------------------\n# B∆Ø·ªöC 2: V√Å FILE MODULES\n# ------------------------------------------------------------------------------\ntarget_file = \"ldm/modules/encoders/modules.py\"\nif os.path.exists(target_file):\n    with open(target_file, \"r\") as f: code = f.read()\n    if \"import hubconf\" not in code:\n        lines = code.split('\\n')\n        for i, line in enumerate(lines):\n            if line.strip().startswith((\"import\", \"from\")):\n                lines.insert(i, \"import os, sys; sys.path.append(os.getcwd()); import hubconf\")\n                break\n        code = '\\n'.join(lines)\n    if \"torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\" in code:\n        code = code.replace(\"torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\", \"hubconf.dinov2_vitg14()\")\n    with open(target_file, \"w\") as f: f.write(code)\n\n# ------------------------------------------------------------------------------\n# B∆Ø·ªöC 3: V√Å L·ªñI NUMPY INT64 (Fix l·ªói n·ªôi t·∫°i c·ªßa th∆∞ vi·ªán Depth)\n# ------------------------------------------------------------------------------\nda_file = \"src/Depth-Anything/metric_depth/zoedepth/models/base_models/depth_anything.py\"\nif os.path.exists(da_file):\n    with open(da_file, \"r\") as f: da_code = f.read()\n    target_str = \"return nn.functional.interpolate(x, (height, width), mode='bilinear', align_corners=True)\"\n    replace_str = \"return nn.functional.interpolate(x, (int(height), int(width)), mode='bilinear', align_corners=True)\"\n    if target_str in da_code:\n        da_code = da_code.replace(target_str, replace_str)\n        with open(da_file, \"w\") as f: f.write(da_code)\n    elif \"(height, width)\" in da_code: # Fallback\n        da_code = da_code.replace(\"(height, width)\", \"(int(height), int(width))\")\n        with open(da_file, \"w\") as f: f.write(da_code)\n\n# ------------------------------------------------------------------------------\n# B∆Ø·ªöC 4: T·∫†O FILE RUN GRADIO (ƒê√£ th√™m l·ªánh resize ƒë·ªìng b·ªô)\n# ------------------------------------------------------------------------------\ngradio_code = r'''\nimport os\nimport sys\nimport torch\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport gradio as gr\n\n# Setup path\nsys.path.append(os.getcwd())\nsys.path.append(os.path.join(os.getcwd(), \"src\", \"grounded_sam\"))\n\n# Import functions\nfrom utils.mpi.preprocess import get_depth_and_sam_mask\nfrom utils.mpi.mpi import get_mpi_rgb_and_alpha\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"‚úÖ Module 1 Environment: {device}\")\n\ndef run_segmentation(input_image, depth_threshold):\n    if input_image is None:\n        raise gr.Error(\"Vui l√≤ng upload ·∫£nh!\")\n    \n    # 1. Resize ·∫£nh ƒë·∫ßu v√†o (Input)\n    w, h = input_image.size\n    max_s = 1024\n    if max(w, h) > max_s:\n        scale = max_s / max(w, h)\n        input_image = input_image.resize((int(w * scale), int(h * scale)), Image.LANCZOS)\n    \n    print(\"running get_depth_and_sam_mask...\")\n    # L·∫•y depth map t·ª´ model (Model c√≥ th·ªÉ tr·∫£ v·ªÅ k√≠ch th∆∞·ªõc h∆°i kh√°c)\n    depth_img, sam_mask_img = get_depth_and_sam_mask(input_image, False)\n    \n    # --- S·ª¨A L·ªñI T·∫†I ƒê√ÇY (QUAN TR·ªåNG) ---\n    # √âp bu·ªôc depth_img ph·∫£i c√≥ k√≠ch th∆∞·ªõc Y H·ªÜT input_image\n    if depth_img.size != input_image.size:\n        print(f\"‚ö†Ô∏è Resizing depth from {depth_img.size} to match input {input_image.size}\")\n        depth_img = depth_img.resize(input_image.size, Image.NEAREST)\n    # ------------------------------------\n\n    rgb_np = np.array(input_image)\n    depth_np = np.array(depth_img)\n    \n    # Ki·ªÉm tra l·∫ßn cu·ªëi\n    if rgb_np.shape[:2] != depth_np.shape[:2]:\n        raise ValueError(f\"Shape mismatch: RGB {rgb_np.shape} vs Depth {depth_np.shape}\")\n\n    # T√°ch l·ªõp\n    thresholds = [(0, depth_threshold), (depth_threshold, 255)]\n    print(f\"running get_mpi_rgb_and_alpha with threshold {depth_threshold}...\")\n    \n    try:\n        mpi_rgb, mpi_alpha = get_mpi_rgb_and_alpha(rgb_np, depth_np, thresholds)\n    except cv2.error as e:\n        print(\"OpenCV Error handled:\", e)\n        # Fallback th·ªß c√¥ng n·∫øu OpenCV v·∫´n l·ªói (hi·∫øm khi x·∫£y ra n·∫øu ƒë√£ resize ·ªü tr√™n)\n        mask = (depth_np > depth_threshold).astype(np.uint8) * 255\n        mpi_rgb = [cv2.bitwise_and(rgb_np, rgb_np, mask=255-mask), cv2.bitwise_and(rgb_np, rgb_np, mask=mask)]\n        mpi_alpha = [(255-mask)/255.0, mask/255.0]\n\n    foreground = mpi_rgb[0]\n    background = mpi_rgb[1]\n    fg_mask = mpi_alpha[0] * 255\n    \n    return [\n        depth_img,\n        sam_mask_img,\n        Image.fromarray(foreground.astype(np.uint8)),\n        Image.fromarray(background.astype(np.uint8)),\n        Image.fromarray(fg_mask.astype(np.uint8))\n    ]\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# üîç Module 1: Segmentation & MPI Splitting\")\n    \n    with gr.Row():\n        with gr.Column(scale=1):\n            input_img = gr.Image(label=\"Input Image\", type=\"pil\", height=400)\n            d_thresh = gr.Slider(0, 255, value=50, step=1, label=\"Depth Threshold\")\n            btn = gr.Button(\"‚úÇÔ∏è T√°ch L·ªõp\", variant=\"primary\")\n        \n        with gr.Column(scale=2):\n            gallery = gr.Gallery(label=\"K·∫øt qu·∫£\", columns=3, height=600, object_fit=\"contain\")\n\n    btn.click(fn=run_segmentation, inputs=[input_img, d_thresh], outputs=[gallery])\n\nif __name__ == \"__main__\":\n    demo.queue().launch(share=True, debug=True, allowed_paths=[\"/kaggle/temp\"])\n'''\n\nwith open(\"gradio_segmentation.py\", \"w\") as f: f.write(gradio_code)\nprint(\"‚úÖ ƒê√£ s·ª≠a l·ªói k√≠ch th∆∞·ªõc ·∫£nh (Size Mismatch Fixed).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T03:28:08.741878Z","iopub.execute_input":"2025-12-25T03:28:08.742428Z","iopub.status.idle":"2025-12-25T03:28:08.781037Z","shell.execute_reply.started":"2025-12-25T03:28:08.742389Z","shell.execute_reply":"2025-12-25T03:28:08.780170Z"}},"outputs":[{"name":"stdout","text":"üöÄ ƒêang thi·∫øt l·∫≠p Module 1 & V√° l·ªói k√≠ch th∆∞·ªõc ·∫£nh...\n‚úÖ ƒê√£ s·ª≠a l·ªói k√≠ch th∆∞·ªõc ·∫£nh (Size Mismatch Fixed).\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ------------------------------------------------------------------------------\n# B∆Ø·ªöC 4: T·∫†O FILE RUN GRADIO (C√ì T√çNH NƒÇNG L∆ØU FILE + RAW DEPTH CHO MODULE 2,4)\n# ------------------------------------------------------------------------------\ngradio_code = r'''\nimport os\nimport sys\nimport torch\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport gradio as gr\nimport matplotlib\nmatplotlib.use('Agg') # Backend kh√¥ng c·∫ßn m√†n h√¨nh\nimport matplotlib.pyplot as plt\nimport io\nimport traceback\n\n# Setup path\nsys.path.append(os.getcwd())\nsys.path.append(os.path.join(os.getcwd(), \"src\", \"grounded_sam\"))\n\nfrom utils.mpi.preprocess import get_depth_and_sam_mask\nfrom utils.mpi.mpi import get_mpi_rgb_and_alpha\n\n# T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥\nOUTPUT_DIR = \"outputs/module1\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(f\"üìÇ K·∫øt qu·∫£ s·∫Ω ƒë∆∞·ª£c l∆∞u t·∫°i: {os.path.abspath(OUTPUT_DIR)}\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --- LOAD ZOEDEPTH (KITTI) ---\nprint(\"‚è≥ ƒêang load model ZoeDepth (ZoeD_K)...\")\nzoe_model = None\ntry:\n    zoe_model = torch.hub.load(\"isl-org/ZoeDepth\", \"ZoeD_K\", pretrained=True).to(device).eval()\n    print(\"‚úÖ Load ZoeDepth KITTI th√†nh c√¥ng!\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è L·ªói load model (s·∫Ω d√πng fallback): {e}\")\n\ndef ensure_pil(img):\n    if isinstance(img, tuple) or isinstance(img, list): img = img[0]\n    if isinstance(img, np.ndarray):\n        if img.dtype != np.uint8:\n            img = (img - img.min()) / (img.max() - img.min() + 1e-5) * 255\n            img = img.astype(np.uint8)\n        img = Image.fromarray(img)\n    if torch.is_tensor(img):\n        img = img.cpu().detach().numpy()\n        img = (img * 255).astype(np.uint8)\n        img = Image.fromarray(img)\n    return img\n\ndef colorize_depth_map(depth_np, is_metric=True):\n    plt.figure(figsize=(10, 8))\n    if isinstance(depth_np, torch.Tensor): depth_np = depth_np.squeeze().cpu().numpy()\n    \n    min_val, max_val = np.min(depth_np), np.max(depth_np)\n    plt.imshow(depth_np, cmap='jet')\n    \n    cbar = plt.colorbar(fraction=0.046, pad=0.04)\n    if is_metric:\n        cbar.set_label('Kho·∫£ng c√°ch th·ª±c t·∫ø (M√©t)', rotation=270, labelpad=20)\n        plt.title(f\"ZoeDepth KITTI (Min: {min_val:.1f}m - Max: {max_val:.1f}m)\")\n    else:\n        cbar.set_label('Gi√° tr·ªã ƒëi·ªÉm ·∫£nh (Relative)', rotation=270, labelpad=20)\n        plt.title(f\"Relative Depth (Colorized)\")\n\n    plt.axis('off')\n    plt.tight_layout()\n    \n    buf = io.BytesIO()\n    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0.1)\n    buf.seek(0)\n    img_color = Image.open(buf).convert(\"RGB\")\n    plt.close()\n    return img_color\n\n# --- C·∫¨P NH·∫¨T: TH√äM THAM S·ªê background_img ---\ndef save_outputs(input_img, mask_img, depth_vis_img, foreground_img, background_img, raw_depth_np):\n    \"\"\"L∆∞u c√°c file c·∫ßn thi·∫øt cho Module 2 v√† Module 4\"\"\"\n    try:\n        # 1. L∆∞u ·∫£nh (Visual)\n        input_img.save(os.path.join(OUTPUT_DIR, \"input.png\"))\n        mask_img.save(os.path.join(OUTPUT_DIR, \"mask.png\"))\n        depth_vis_img.save(os.path.join(OUTPUT_DIR, \"depth_vis.png\"))\n        foreground_img.save(os.path.join(OUTPUT_DIR, \"foreground.png\"))\n        \n        # --- L∆ØU BACKGROUND ---\n        background_img.save(os.path.join(OUTPUT_DIR, \"background.png\"))\n        \n        # 2. L∆ØU QUAN TR·ªåNG: RAW DEPTH (NUMPY)\n        # File n√†y c·∫ßn cho Module 4 t√≠nh to√°n 3D\n        np.save(os.path.join(OUTPUT_DIR, \"depth.npy\"), raw_depth_np)\n        \n        print(f\"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ (bao g·ªìm background.png v√† depth.npy) v√†o {OUTPUT_DIR}\")\n    except Exception as e:\n        print(f\"‚ùå L·ªói khi l∆∞u file: {e}\")\n        import traceback\n        traceback.print_exc()\n\ndef run_segmentation(input_image, depth_threshold):\n    if input_image is None: raise gr.Error(\"Ch∆∞a upload ·∫£nh!\")\n    \n    # Resize v·ªÅ chu·∫©n Model (tr√°nh qu√° to ho·∫∑c l·∫ª)\n    w, h = input_image.size\n    max_s = 1024\n    if max(w, h) > max_s:\n        scale = max_s / max(w, h)\n        input_image = input_image.resize((int(w * scale), int(h * scale)), Image.LANCZOS)\n    \n    w, h = input_image.size # C·∫≠p nh·∫≠t l·∫°i size\n    \n    # Bi·∫øn l∆∞u tr·ªØ depth th√¥ (ƒë·ªÉ l∆∞u file .npy)\n    final_raw_depth = None \n    depth_metric_img = None\n    \n    # 1. Metric Depth (ZoeDepth)\n    if zoe_model is not None:\n        try:\n            with torch.no_grad():\n                depth_tensor = zoe_model.infer_pil(input_image)\n            if isinstance(depth_tensor, torch.Tensor): raw_metric = depth_tensor.cpu().numpy()\n            else: raw_metric = depth_tensor\n            \n            # Resize depth v·ªÅ ƒë√∫ng size ·∫£nh input (n·∫øu Zoe tr·∫£ v·ªÅ size kh√°c)\n            if raw_metric.shape[-2:] != (h, w):\n                 raw_metric = cv2.resize(raw_metric, (w, h), interpolation=cv2.INTER_LINEAR)\n            \n            final_raw_depth = raw_metric # L∆∞u l·∫°i depth th√¥ n√†y\n            depth_metric_img = colorize_depth_map(raw_metric, is_metric=True)\n        except Exception as e:\n            print(f\"Zoe Error: {e}\")\n            depth_metric_img = None\n\n    # 2. Segmentation (SAM / Pretrained)\n    print(\"üöÄ Running Segmentation...\")\n    raw_results = get_depth_and_sam_mask(input_image, False)\n    depth_raw_rel = raw_results[0]\n    sam_mask_raw = raw_results[1]\n    \n    depth_img_rel = ensure_pil(depth_raw_rel)\n    sam_mask_img = ensure_pil(sam_mask_raw)\n    \n    # Resize mask v·ªÅ ƒë√∫ng size ·∫£nh input\n    if sam_mask_img.size != input_image.size:\n        sam_mask_img = sam_mask_img.resize(input_image.size, Image.NEAREST)\n\n    # Fallback Depth: N·∫øu Zoe l·ªói, d√πng depth t∆∞∆°ng ƒë·ªëi\n    if final_raw_depth is None:\n        final_raw_depth = np.array(depth_img_rel).astype(np.float32) / 255.0 # Chu·∫©n h√≥a v·ªÅ 0-1\n        if depth_metric_img is None:\n            depth_metric_img = colorize_depth_map(np.array(depth_img_rel), is_metric=False)\n\n    # 3. MPI / Cutout (T√°ch n·ªÅn d·ª±a tr√™n ng∆∞·ª°ng)\n    rgb_np = np.array(input_image)\n    depth_np_rel = np.array(depth_img_rel)\n    \n    # Convert mask SAM sang binary (0-255)\n    sam_mask_np = np.array(sam_mask_img)\n    if len(sam_mask_np.shape) == 3: sam_mask_np = sam_mask_np[:,:,0]\n    \n    # T·∫°o ·∫£nh Foreground (Ch·ªâ gi·ªØ l·∫°i ph·∫ßn trong mask)\n    fg_rgba = np.zeros((h, w, 4), dtype=np.uint8)\n    fg_rgba[:, :, :3] = rgb_np\n    fg_rgba[:, :, 3] = sam_mask_np # Alpha channel\n    \n    fg_img = Image.fromarray(fg_rgba)\n    \n    # T·∫°o ·∫£nh Background (Ph·∫ßn c√≤n l·∫°i)\n    bg_rgba = np.zeros((h, w, 4), dtype=np.uint8)\n    bg_rgba[:, :, :3] = rgb_np\n    bg_rgba[:, :, 3] = 255 - sam_mask_np\n    bg_img = Image.fromarray(bg_rgba)\n    \n    alpha_img = Image.fromarray(sam_mask_np)\n\n    # --- L∆ØU OUTPUT QUAN TR·ªåNG (ƒê√É TH√äM BG_IMG) ---\n    save_outputs(input_image, sam_mask_img, depth_metric_img, fg_img, bg_img, final_raw_depth)\n    # -------------------------------------------\n\n    return [depth_metric_img, sam_mask_img, fg_img, bg_img, alpha_img]\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# üîç Module 1: Outdoor Segmentation & ZoeDepth (Fixed Auto-Save)\")\n    gr.Markdown(\"K·∫øt qu·∫£ (bao g·ªìm depth map th√¥) s·∫Ω t·ª± ƒë·ªông l∆∞u v√†o `outputs/module1`.\")\n    \n    with gr.Row():\n        with gr.Column(scale=1):\n            input_img = gr.Image(label=\"Input Image\", type=\"pil\", height=400)\n            d_thresh = gr.Slider(0, 255, value=100, step=1, label=\"Ng∆∞·ª°ng t√°ch l·ªõp (Kh√¥ng d√πng n·∫øu c√≥ SAM)\")\n            btn = gr.Button(\"üöÄ X·ª≠ l√Ω & L∆∞u\", variant=\"primary\")\n        \n        with gr.Column(scale=2):\n            gallery = gr.Gallery(label=\"K·∫øt qu·∫£ (Depth, Mask, FG, BG, Alpha)\", columns=3, height=600, object_fit=\"contain\")\n\n    btn.click(fn=run_segmentation, inputs=[input_img, d_thresh], outputs=[gallery])\n\nif __name__ == \"__main__\":\n    demo.queue().launch(share=True, debug=True, allowed_paths=[\"/kaggle/temp\"])\n'''\n\nwith open(\"gradio_segmentation.py\", \"w\") as f: f.write(gradio_code)\nprint(\"‚úÖ ƒê√£ c·∫≠p nh·∫≠t Gradio: ƒê√£ s·ª≠a l·ªói l∆∞u thi·∫øu background.png.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T03:28:08.782075Z","iopub.execute_input":"2025-12-25T03:28:08.782747Z","iopub.status.idle":"2025-12-25T03:28:15.274242Z","shell.execute_reply.started":"2025-12-25T03:28:08.782716Z","shell.execute_reply":"2025-12-25T03:28:15.273409Z"}},"outputs":[{"name":"stdout","text":"‚úÖ ƒê√£ c·∫≠p nh·∫≠t Gradio: ƒê√£ s·ª≠a l·ªói l∆∞u thi·∫øu background.png.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# @title 6Ô∏è‚É£ RUN MODULE 1 (WEIGHTS FIX): T·∫£i Weights & Ch·∫°y 10 ·∫£nh\nimport os\nimport sys\nimport gc\nimport glob\nimport shutil\nimport ctypes\nimport logging\nimport numpy as np\nimport torch\nimport subprocess\nfrom PIL import Image\n\n# T·ª± ƒë·ªông c√†i pyspark\ntry:\n    from pyspark.sql import SparkSession\nexcept ImportError:\n    os.system(\"pip install pyspark\")\n    from pyspark.sql import SparkSession\n\n# ==============================================================================\n# üõ†Ô∏è B∆Ø·ªöC 1: T·ª∞ ƒê·ªòNG T·∫¢I WEIGHTS (QUAN TR·ªåNG)\n# ==============================================================================\nPROJECT_DIR = \"/kaggle/working/Depth-Aware-Editing\"\nWEIGHTS_DIR = os.path.join(PROJECT_DIR, \"weights\")\n\nprint(f\"üîß ƒêang ki·ªÉm tra weights t·∫°i: {WEIGHTS_DIR}\")\n\n# Ki·ªÉm tra xem file b·ªã thi·∫øu c√≥ t·ªìn t·∫°i kh√¥ng\nMISSING_FILE = \"depth_anything_metric_depth_indoor.pt\"\nif not os.path.exists(os.path.join(WEIGHTS_DIR, MISSING_FILE)):\n    print(f\"‚ö†Ô∏è Ph√°t hi·ªán thi·∫øu file: {MISSING_FILE}\")\n    print(\"üîÑ ƒêang ch·∫°y script 'download_weights.sh'...\")\n    \n    # Ch·∫°y l·ªánh bash ƒë·ªÉ t·∫£i\n    try:\n        # C·∫•p quy·ªÅn th·ª±c thi\n        subprocess.run([\"chmod\", \"+x\", os.path.join(PROJECT_DIR, \"download_weights.sh\")], check=True)\n        # Ch·∫°y script trong th∆∞ m·ª•c d·ª± √°n\n        subprocess.run([\"bash\", \"download_weights.sh\"], cwd=PROJECT_DIR, check=True)\n        print(\"‚úÖ ƒê√£ t·∫£i xong weights!\")\n    except Exception as e:\n        print(f\"‚ùå L·ªói khi t·∫£i weights: {e}\")\n        # Fallback: T·ª± t·∫£i th·ªß c√¥ng n·∫øu script l·ªói (Link d·ª± ph√≤ng)\n        if not os.path.exists(WEIGHTS_DIR): os.makedirs(WEIGHTS_DIR)\n        print(\"üåç ƒêang th·ª≠ t·∫£i th·ªß c√¥ng t·ª´ HuggingFace...\")\n        url = \"https://huggingface.co/spaces/LiheYoung/Depth-Anything/resolve/main/checkpoints/depth_anything_metric_depth_indoor.pt\"\n        os.system(f\"wget {url} -P {WEIGHTS_DIR}\")\n\n# ==============================================================================\n# 1. C·∫§U H√åNH\n# ==============================================================================\nINPUT_FOLDER = \"/kaggle/input/kitti-3d-object-detection-dataset\"\nOUTPUT_BASE_DIR = \"/kaggle/working/Depth-Aware-Editing/spark_output/module1\"\nTEST_LIMIT = 50 \nSPARK_MASTER = \"local[1]\" \n\ndef clean_memory():\n    cleanup_vars = ['model', 'zoe_model', 'pipe', 'diff_handles', 'depth_anything']\n    for var in cleanup_vars:\n        if var in globals(): del globals()[var]\n    gc.collect()\n    try: ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n    except: pass\n    if torch.cuda.is_available(): torch.cuda.empty_cache()\n\n# ==============================================================================\n# 2. LOGIC WORKER (C√ì FIX CWD & PATH)\n# ==============================================================================\ndef spark_worker_logic(iterator):\n    import os\n    import sys\n    import torch\n    import cv2\n    import numpy as np\n    from PIL import Image\n    import logging\n\n    # --- üî• FIX 1: CHUY·ªÇN CWD V·ªÄ ƒê√öNG CH·ªñ üî• ---\n    # Code c·ªßa b·∫°n d√πng ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi \"./weights/...\", \n    # n√™n ta ph·∫£i b·∫Øt Worker ƒë·ª©ng ƒë√∫ng t·∫°i th∆∞ m·ª•c d·ª± √°n.\n    os.chdir(\"/kaggle/working/Depth-Aware-Editing\")\n    # -------------------------------------------\n\n    # --- üî• FIX 2: N·∫†P PATH (Nh∆∞ c≈©) ---\n    base_dir = os.getcwd() # L√∫c n√†y l√† .../Depth-Aware-Editing\n    src_dir = os.path.join(base_dir, \"src\")\n    \n    paths_to_inject = [\n        os.path.join(src_dir, \"ZoeDepth\"),\n        os.path.join(src_dir, \"grounded_sam\"),\n        src_dir,\n        base_dir\n    ]\n    for p in paths_to_inject:\n        if os.path.exists(p) and p not in sys.path:\n            sys.path.insert(0, p)\n    # -----------------------------------\n\n    # T·∫Øt log\n    try:\n        from transformers import logging as trans_logging\n        trans_logging.set_verbosity_error()\n        logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n    except: pass\n\n    # Import\n    try:\n        from utils.mpi.preprocess import get_depth_and_sam_mask\n    except ImportError as e:\n        yield (\"IMPORT_ERROR\", str(e), f\"CWD: {os.getcwd()}\")\n        return\n\n    # Load Model ZoeDepth (N·∫øu code trong preprocess kh√¥ng t·ª± load)\n    # L∆∞u √Ω: preprocess.py c·ªßa b·∫°n d∆∞·ªùng nh∆∞ t·ª± g·ªçi get_depth_map -> t·ª± load model\n    # n√™n ta kh√¥ng c·∫ßn load th·ªß c√¥ng ·ªü ƒë√¢y n·ªØa ƒë·ªÉ ti·∫øt ki·ªám VRAM.\n    \n    # Loop x·ª≠ l√Ω\n    for row in iterator:\n        image_path = row.image_path\n        img_id = os.path.basename(image_path).split('.')[0]\n        \n        try:\n            # 1. ƒê·ªçc ·∫£nh\n            input_image = Image.open(image_path).convert(\"RGB\")\n            w, h = input_image.size\n            \n            # Resize\n            max_s = 1024\n            if max(w, h) > max_s:\n                scale = max_s / max(w, h)\n                input_image = input_image.resize((int(w * scale), int(h * scale)), Image.LANCZOS)\n                w, h = input_image.size\n            \n            # 2. CH·∫†Y CODE G·ªêC (ƒê√£ fix weights)\n            # H√†m n√†y s·∫Ω t·ª± load model t·ª´ ./weights/depth_anything_...\n            depth_img_rel, sam_mask_img = get_depth_and_sam_mask(input_image, False)\n            \n            # 3. Chu·∫©n h√≥a Depth (N·∫øu h√†m tr·∫£ v·ªÅ PIL Image)\n            final_depth = np.array(depth_img_rel).astype(np.float32) / 255.0\n            \n            # 4. L∆∞u k·∫øt qu·∫£\n            save_dir = os.path.join(OUTPUT_BASE_DIR, img_id)\n            os.makedirs(save_dir, exist_ok=True)\n            \n            # Mask\n            sam_mask_img = sam_mask_img.resize((w, h), Image.NEAREST)\n            sam_mask_np = np.array(sam_mask_img)\n            if len(sam_mask_np.shape) == 3: sam_mask_np = sam_mask_np[:,:,0]\n            \n            # RGBA\n            rgb_np = np.array(input_image)\n            fg_rgba = np.zeros((h, w, 4), dtype=np.uint8)\n            fg_rgba[:, :, :3] = rgb_np\n            fg_rgba[:, :, 3] = sam_mask_np\n            \n            bg_rgba = np.zeros((h, w, 4), dtype=np.uint8)\n            bg_rgba[:, :, :3] = rgb_np\n            bg_rgba[:, :, 3] = 255 - sam_mask_np\n\n            Image.fromarray(fg_rgba).save(os.path.join(save_dir, \"foreground.png\"))\n            Image.fromarray(bg_rgba).save(os.path.join(save_dir, \"background.png\"))\n            Image.fromarray(sam_mask_np).save(os.path.join(save_dir, \"mask.png\"))\n            input_image.save(os.path.join(save_dir, \"input.png\"))\n            np.save(os.path.join(save_dir, \"depth.npy\"), final_depth)\n            \n            yield (img_id, \"SUCCESS\", save_dir)\n            \n        except Exception as e:\n            yield (img_id, f\"ERROR: {str(e)}\", \"\")\n\n# ==============================================================================\n# 3. SPARK DRIVER\n# ==============================================================================\ndef main_spark_job():\n    clean_memory()\n    print(f\"üöÄ Kh·ªüi t·∫°o Spark Session ({SPARK_MASTER})...\")\n    spark = SparkSession.builder \\\n        .appName(\"Module1_WeightsFix\") \\\n        .config(\"spark.driver.memory\", \"4g\") \\\n        .master(SPARK_MASTER) \\\n        .getOrCreate()\n    \n    # Qu√©t ·∫£nh\n    image_files = []\n    for ext in ('*.jpg', '*.jpeg', '*.png'):\n        image_files.extend(glob.glob(os.path.join(INPUT_FOLDER, \"**\", ext), recursive=True))\n    \n    if len(image_files) > TEST_LIMIT:\n        print(f\"‚ö†Ô∏è Test {TEST_LIMIT} ·∫£nh.\")\n        image_files = image_files[:TEST_LIMIT]\n        \n    if not image_files:\n        print(\"‚ùå Kh√¥ng t√¨m th·∫•y ·∫£nh!\")\n        spark.stop()\n        return\n\n    df = spark.createDataFrame([(f,) for f in image_files], [\"image_path\"])\n    df_partitioned = df.repartition(1)\n    \n    print(f\"üîÑ ƒêang x·ª≠ l√Ω {len(image_files)} ·∫£nh...\")\n    results_rdd = df_partitioned.rdd.mapPartitions(spark_worker_logic)\n    final_results = results_rdd.collect()\n    \n    print(\"\\n‚úÖ K·∫æT QU·∫¢ X·ª¨ L√ù:\")\n    success_count = 0\n    for img_id, status, note in final_results:\n        if status == \"SUCCESS\": \n            success_count += 1\n            print(f\"{img_id[:15]:<15} | {status:<10} | OK\")\n        else:\n            print(f\"{img_id[:15]:<15} | {status:<10} | {note[:40]}...\")\n            \n    print(f\"üéâ Ho√†n th√†nh: {success_count}/{len(final_results)}\")\n    spark.stop()\n\nif __name__ == \"__main__\":\n    if os.path.exists(OUTPUT_BASE_DIR): shutil.rmtree(OUTPUT_BASE_DIR)\n    os.makedirs(OUTPUT_BASE_DIR, exist_ok=True)\n    main_spark_job()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T03:28:15.276010Z","iopub.execute_input":"2025-12-25T03:28:15.276241Z","iopub.status.idle":"2025-12-25T03:37:07.726870Z","shell.execute_reply.started":"2025-12-25T03:28:15.276221Z","shell.execute_reply":"2025-12-25T03:37:07.726106Z"}},"outputs":[{"name":"stdout","text":"üîß ƒêang ki·ªÉm tra weights t·∫°i: /kaggle/working/Depth-Aware-Editing/weights\nüöÄ Kh·ªüi t·∫°o Spark Session (local[1])...\n","output_type":"stream"},{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/12/25 03:28:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"},{"name":"stdout","text":"‚ö†Ô∏è Test 50 ·∫£nh.\nüîÑ ƒêang x·ª≠ l√Ω 50 ·∫£nh...\n","output_type":"stream"},{"name":"stderr","text":"xFormers not available                                              (0 + 1) / 1]\nxFormers not available\nParams passed to Resize transform:\n\twidth:  518\n\theight:  392\n\tresize_target:  True\n\tkeep_aspect_ratio:  False\n\tensure_multiple_of:  14\n\tresize_method:  minimal\nUsing pretrained resource local::./weights/depth_anything_metric_depth_indoor.pt\nLoaded successfully\n/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ K·∫æT QU·∫¢ X·ª¨ L√ù:\n005084          | SUCCESS    | OK\n003065          | SUCCESS    | OK\n003741          | SUCCESS    | OK\n004725          | SUCCESS    | OK\n001183          | SUCCESS    | OK\n005660          | SUCCESS    | OK\n007355          | SUCCESS    | OK\n003244          | SUCCESS    | OK\n002594          | SUCCESS    | OK\n001703          | SUCCESS    | OK\n003380          | SUCCESS    | OK\n007190          | SUCCESS    | OK\n004566          | SUCCESS    | OK\n002584          | SUCCESS    | OK\n004035          | SUCCESS    | OK\n001840          | SUCCESS    | OK\n005647          | SUCCESS    | OK\n004924          | SUCCESS    | OK\n005000          | SUCCESS    | OK\n007456          | SUCCESS    | OK\n006796          | SUCCESS    | OK\n000419          | SUCCESS    | OK\n002712          | SUCCESS    | OK\n006386          | SUCCESS    | OK\n000920          | SUCCESS    | OK\n000585          | SUCCESS    | OK\n001593          | SUCCESS    | OK\n004178          | SUCCESS    | OK\n002011          | SUCCESS    | OK\n007203          | SUCCESS    | OK\n004710          | SUCCESS    | OK\n002312          | SUCCESS    | OK\n003141          | SUCCESS    | OK\n003053          | SUCCESS    | OK\n006833          | SUCCESS    | OK\n004278          | SUCCESS    | OK\n005398          | SUCCESS    | OK\n000313          | SUCCESS    | OK\n002084          | SUCCESS    | OK\n000603          | SUCCESS    | OK\n007284          | SUCCESS    | OK\n001669          | SUCCESS    | OK\n004656          | SUCCESS    | OK\n007141          | SUCCESS    | OK\n005328          | SUCCESS    | OK\n003381          | SUCCESS    | OK\n004772          | SUCCESS    | OK\n005847          | SUCCESS    | OK\n005504          | SUCCESS    | OK\n005092          | SUCCESS    | OK\nüéâ Ho√†n th√†nh: 50/50\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\n\n# T·∫°o th∆∞ m·ª•c ch·ª©a code\nos.makedirs(\"src/featglac\", exist_ok=True)\n\n# N·ªôi dung file code (ƒê√£ th√™m PATCH FIX cho PyTorch 2.1.2)\ninverter_code = r'''\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom typing import Optional, Union, Tuple, List\n\n# --- üî• HOTFIX: V√Å L·ªñI COMPATIBILITY PYTORCH 2.1.2 ---\n# L·ªói b·∫°n g·∫∑p l√† do accelerate g·ªçi h√†m c≈© 'grad_and_value' kh√¥ng c√≤n trong torch 2.1\n# Ta s·∫Ω ƒë·ªãnh nghƒ©a th·ªß c√¥ng n√≥ tr·ªè v·ªÅ 'torch.func.grad_and_value' (API m·ªõi)\ntry:\n    import torch._functorch.eager_transforms\n    if not hasattr(torch._functorch.eager_transforms, \"grad_and_value\"):\n        print(\"üõ†Ô∏è ƒêang √°p d·ª•ng b·∫£n v√°: torch._functorch.eager_transforms.grad_and_value\")\n        torch._functorch.eager_transforms.grad_and_value = torch.func.grad_and_value\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è C·∫£nh b√°o Patching: {e}\")\n# -----------------------------------------------------\n\nclass StableNullInverter:\n    def __init__(self, pipe):\n        self.pipe = pipe\n        self.tokenizer = pipe.tokenizer\n        self.text_encoder = pipe.text_encoder\n        self.vae = pipe.vae\n        self.unet = pipe.unet\n        self.scheduler = pipe.scheduler\n        self.device = pipe.device\n\n    @torch.no_grad()\n    def image2latent(self, image: Image.Image) -> torch.Tensor:\n        \"\"\"Chuy·ªÉn ·∫£nh PIL RGB -> Latent Tensor\"\"\"\n        # Resize v·ªÅ 512x512\n        image = image.resize((512, 512), resample=Image.Resampling.LANCZOS)\n        \n        # Normalize [-1, 1]\n        img_np = np.array(image).astype(np.float32) / 127.5 - 1.0\n        \n        # To Tensor [B, C, H, W]\n        img_tensor = torch.from_numpy(img_np).permute(2, 0, 1).unsqueeze(0)\n        img_tensor = img_tensor.to(self.device).to(self.pipe.dtype)\n        \n        # Encode VAE\n        latents = self.vae.encode(img_tensor).latent_dist.mean\n        latents = latents * 0.18215\n        return latents\n\n    @torch.no_grad()\n    def invert_input_image(\n        self, \n        image: Image.Image, \n        prompt: str = \"\", \n        num_inference_steps: int = 50,\n        guidance_scale: float = 1.0\n    ) -> Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:\n        \"\"\"Th·ª±c hi·ªán DDIM Inversion\"\"\"\n        \n        # 1. Encode ·∫£nh g·ªëc\n        latents = self.image2latent(image)\n        start_latents = latents.clone()\n        \n        # 2. Text Embeddings\n        text_input = self.tokenizer(\n            [prompt], \n            padding=\"max_length\", \n            max_length=self.tokenizer.model_max_length, \n            truncation=True, \n            return_tensors=\"pt\"\n        )\n        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n\n        # 3. Scheduler Setup\n        self.scheduler.set_timesteps(num_inference_steps)\n        reversed_timesteps = self.scheduler.timesteps\n        if reversed_timesteps[0] > reversed_timesteps[-1]:\n            reversed_timesteps = torch.flip(reversed_timesteps, [0])\n\n        print(f\"üîÑ ƒêang Invert ({num_inference_steps} steps)...\")\n        \n        # L∆∞u qu·ªπ ƒë·∫°o (CPU ƒë·ªÉ ti·∫øt ki·ªám VRAM)\n        ddim_latents_all = [latents.cpu()] \n\n        # 4. Inversion Loop\n        for i, t in enumerate(tqdm(reversed_timesteps)):\n            noise_pred = self.unet(\n                latents, \n                t, \n                encoder_hidden_states=text_embeddings\n            ).sample\n\n            alpha_prod_t = self.scheduler.alphas_cumprod[t]\n            beta_prod_t = 1 - alpha_prod_t\n            \n            if i < len(reversed_timesteps) - 1:\n                next_t = reversed_timesteps[i+1]\n                alpha_prod_t_next = self.scheduler.alphas_cumprod[next_t]\n            else:\n                alpha_prod_t_next = self.scheduler.alphas_cumprod[0] \n\n            # DDIM Formula\n            pred_original_sample = (latents - beta_prod_t ** 0.5 * noise_pred) / (alpha_prod_t ** 0.5)\n            latents = alpha_prod_t_next ** 0.5 * pred_original_sample + (1 - alpha_prod_t_next) ** 0.5 * noise_pred\n            \n            ddim_latents_all.append(latents.cpu())\n\n        print(\"‚úÖ Inversion ho√†n t·∫•t.\")\n        return start_latents, latents, ddim_latents_all\n'''\n\nwith open(\"src/featglac/stable_null_inverter.py\", \"w\") as f:\n    f.write(inverter_code)\nprint(\"‚úÖ ƒê√£ t·∫°o file th∆∞ vi·ªán (K√®m b·∫£n v√° l·ªói PyTorch 2.1).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T03:37:07.728717Z","iopub.execute_input":"2025-12-25T03:37:07.729260Z","iopub.status.idle":"2025-12-25T03:37:07.737431Z","shell.execute_reply.started":"2025-12-25T03:37:07.729230Z","shell.execute_reply":"2025-12-25T03:37:07.736685Z"}},"outputs":[{"name":"stdout","text":"‚úÖ ƒê√£ t·∫°o file th∆∞ vi·ªán (K√®m b·∫£n v√° l·ªói PyTorch 2.1).\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# @title üöë EMERGENCY PATCH: B·ªè qua l·ªói FSDP & Torch Distributed\nimport sys\nfrom unittest.mock import MagicMock\nimport torch\n\nprint(\"üöë ƒêANG V√Å L·ªñI XUNG ƒê·ªòT PYTORCH & ACCELERATE...\")\n\n# 1. Ch·∫∑n import module g√¢y l·ªói trong Torch\n# L·ªói '_fused_rms_norm_backward' n·∫±m trong torch.distributed.tensor._ops\n# Ta s·∫Ω thay th·∫ø to√†n b·ªô module n√†y b·∫±ng Mock (ƒë·ªì gi·∫£) ƒë·ªÉ Python kh√¥ng ch·∫°y v√†o ƒë√≥.\nsys.modules[\"torch.distributed.tensor._ops\"] = MagicMock()\nsys.modules[\"torch.distributed.tensor\"] = MagicMock()\nsys.modules[\"torch.distributed.checkpoint\"] = MagicMock()\nsys.modules[\"torch.distributed.fsdp\"] = MagicMock()\n\n# 2. Ch·∫∑n Accelerate g·ªçi FSDP Utils\nsys.modules[\"accelerate.utils.fsdp_utils\"] = MagicMock()\n\n# 3. V√° l·∫°i c√°c l·ªói c≈© (n·∫øu ch∆∞a ch·∫°y)\ntry:\n    if not hasattr(torch.ops.aten, \"_fused_rms_norm_backward\"):\n        # T·∫°o dummy attribute ƒë·ªÉ n·∫øu c√≥ module n√†o l·ªçt l∆∞·ªõi ki·ªÉm tra th√¨ kh√¥ng b·ªã crash\n        setattr(torch.ops.aten, \"_fused_rms_norm_backward\", MagicMock())\nexcept:\n    pass\n\n# 4. Patch Grad_and_value (cho Inversion)\ntry:\n    import torch._functorch.eager_transforms\n    if not hasattr(torch._functorch.eager_transforms, \"grad_and_value\"):\n        torch._functorch.eager_transforms.grad_and_value = torch.func.grad_and_value\nexcept: \n    pass\n\nprint(\"‚úÖ ƒê√É V√Å XONG! B·∫†N C√ì TH·ªÇ CH·∫†Y TI·∫æP CELL B√äN D∆Ø·ªöI.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T03:37:07.739100Z","iopub.execute_input":"2025-12-25T03:37:07.739707Z","iopub.status.idle":"2025-12-25T03:37:07.761928Z","shell.execute_reply.started":"2025-12-25T03:37:07.739683Z","shell.execute_reply":"2025-12-25T03:37:07.761218Z"}},"outputs":[{"name":"stdout","text":"üöë ƒêANG V√Å L·ªñI XUNG ƒê·ªòT PYTORCH & ACCELERATE...\n‚úÖ ƒê√É V√Å XONG! B·∫†N C√ì TH·ªÇ CH·∫†Y TI·∫æP CELL B√äN D∆Ø·ªöI.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# @title 7Ô∏è‚É£ RUN MODULE 2 (FIXED PATH): S·ª≠a l·ªói thi·∫øu StableNullInverter\nimport os\nimport sys\nimport gc\nimport glob\nimport time\nimport shutil\nimport types\nimport numpy as np\nimport torch\nimport pandas as pd\nfrom PIL import Image\nfrom unittest.mock import MagicMock\nfrom pyspark.sql import SparkSession\n\n# ================= C·∫§U H√åNH MODULE 2 =================\nINPUT_DIR = \"/kaggle/working/Depth-Aware-Editing/spark_output/module1\"\nOUTPUT_DIR = \"/kaggle/working/Depth-Aware-Editing/spark_output/module2\" \nNUM_INFERENCE_STEPS = 20\nSPARK_MASTER = \"local[1]\" \n\n# ================= üöë EMERGENCY PATCH: FIX L·ªñI TORCH.MTIA =================\nif not hasattr(torch, 'mtia'):\n    torch.mtia = types.ModuleType('mtia')\nif not hasattr(torch.mtia, '_set_stream_by_id'):\n    torch.mtia._set_stream_by_id = lambda *args, **kwargs: None\nif not hasattr(torch.mtia, 'set_stream'):\n    torch.mtia.set_stream = lambda *args, **kwargs: None\nif not hasattr(torch.mtia, 'synchronize'):\n    torch.mtia.synchronize = lambda *args, **kwargs: None\nif not hasattr(torch.mtia, 'device'):\n    class FakeDevice: pass\n    torch.mtia.device = FakeDevice\n\n# ================= H√ÄM H·ªñ TR·ª¢ DRIVER =================\ndef clean_memory():\n    cleanup_vars = ['pipe', 'inverter', 'zoe_model', 'unet', 'vae']\n    for var in cleanup_vars:\n        if var in globals(): del globals()[var]\n    gc.collect()\n    if torch.cuda.is_available(): torch.cuda.empty_cache()\n\n# ================= LOGIC WORKER (C√ì FIX PATH) =================\ndef inversion_worker(iterator):\n    import os\n    import sys\n    import torch\n    import time\n    import numpy as np\n    from PIL import Image\n    from diffusers import StableDiffusionPipeline \n    \n    # --- üî• FIX QUAN TR·ªåNG: √âP ƒê∆Ø·ªúNG D·∫™N CODE ---\n    base_dir = \"/kaggle/working/Depth-Aware-Editing\"\n    \n    # N·∫°p c·∫£ th∆∞ m·ª•c g·ªëc v√† th∆∞ m·ª•c src v√†o sys.path\n    paths_to_inject = [\n        base_dir,                       # ƒê·ªÉ import 'src.featglac...'\n        os.path.join(base_dir, \"src\")   # D·ª± ph√≤ng\n    ]\n    \n    for p in paths_to_inject:\n        if os.path.exists(p) and p not in sys.path:\n            sys.path.insert(0, p)\n    # ---------------------------------------------\n\n    try:\n        # Import class Inverter t·ª´ code d·ª± √°n\n        from src.featglac.stable_null_inverter import StableNullInverter\n    except ImportError as e:\n        # N·∫øu l·ªói, tr·∫£ v·ªÅ th√¥ng b√°o debug chi ti·∫øt\n        yield (\"INIT_ERROR\", f\"Missing StableNullInverter: {str(e)}\", 0, 0)\n        return\n\n    # 1. Load Model SD v1.5\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model_id = \"runwayml/stable-diffusion-v1-5\"\n    \n    try:\n        pipe = StableDiffusionPipeline.from_pretrained(\n            model_id,\n            torch_dtype=torch.float16,\n            safety_checker=None\n        ).to(device)\n        \n        # Kh·ªüi t·∫°o Inverter\n        inverter = StableNullInverter(pipe)\n    except Exception as e:\n        yield (\"MODEL_LOAD_ERROR\", str(e), 0, 0)\n        return\n\n    # 2. Loop x·ª≠ l√Ω ·∫£nh\n    for row in iterator:\n        img_id = row.img_id\n        img_path = row.img_path\n        target_output_dir = row.output_dir \n        \n        start_time = time.time()\n        \n        try:\n            # T·∫°o th∆∞ m·ª•c output ri√™ng cho ·∫£nh n√†y\n            save_folder = os.path.join(target_output_dir, img_id)\n            os.makedirs(save_folder, exist_ok=True)\n            \n            latent_path = os.path.join(save_folder, \"inverted_latents.pt\")\n            \n            # Check t·ªìn t·∫°i ƒë·ªÉ skip (Resume capability)\n            if os.path.exists(latent_path):\n                yield (img_id, \"SKIPPED\", 0, 0)\n                continue\n\n            # Load ·∫£nh input t·ª´ Module 1\n            input_image = Image.open(img_path).convert(\"RGB\").resize((512, 512))\n            \n            # --- TH·ª∞C HI·ªÜN INVERSION ---\n            with torch.autocast(\"cuda\"):\n                # H√†m invert tr·∫£ v·ªÅ (start_latents, inverted_latents, all_latents)\n                # Ta c·∫ßn l∆∞u start_latents (z_T) ƒë·ªÉ d√πng cho Module 3\n                _, inverted_latents, _ = inverter.invert_input_image(\n                    input_image, \n                    prompt=\"\", \n                    num_inference_steps=NUM_INFERENCE_STEPS\n                )\n            \n            # L∆∞u file k·∫øt qu·∫£ .pt\n            torch.save(inverted_latents.cpu(), latent_path)\n            \n            # --- T√çNH TO√ÅN SAI S·ªê (MSE) ---\n            # Reconstruct ·∫£nh ƒë·ªÉ ƒëo ƒë·ªô sai l·ªách\n            with torch.no_grad():\n                z0 = inverter.image2latent(input_image)\n                recon_img_tensor = pipe.vae.decode(z0 / 0.18215).sample\n                recon_img = (recon_img_tensor / 2 + 0.5).clamp(0, 1).cpu().permute(0, 2, 3, 1).numpy()[0]\n                \n            orig_img = np.array(input_image).astype(np.float32) / 255.0\n            mse = np.mean((orig_img - recon_img) ** 2)\n            \n            process_time = time.time() - start_time\n            yield (img_id, \"SUCCESS\", round(process_time, 2), round(mse, 6))\n            \n        except Exception as e:\n            yield (img_id, f\"ERROR: {str(e)}\", 0, 0)\n\n# ================= MAIN DRIVER =================\ndef main_module2():\n    clean_memory()\n    print(f\"üöÄ Kh·ªüi ch·∫°y Spark Module 2 (Inversion) - Output: {OUTPUT_DIR}\")\n    \n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    \n    spark = SparkSession.builder \\\n        .appName(\"Module2_Inversion_Fixed\") \\\n        .config(\"spark.driver.memory\", \"4g\") \\\n        .config(\"spark.executor.memory\", \"4g\") \\\n        .master(SPARK_MASTER) \\\n        .getOrCreate()\n    \n    # 1. Qu√©t file input t·ª´ Module 1 (File 'input.png' do Module 1 t·∫°o ra)\n    # C·∫•u tr√∫c Module 1: spark_output/module1/{img_id}/input.png\n    search_pattern = os.path.join(INPUT_DIR, \"*\", \"input.png\")\n    input_files = glob.glob(search_pattern)\n    \n    if not input_files:\n        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y ·∫£nh input n√†o t·∫°i: {INPUT_DIR}\")\n        print(\"üëâ H√£y ch·∫Øc ch·∫Øn Module 1 ƒë√£ ch·∫°y th√†nh c√¥ng v√† t·∫°o file input.png\")\n        spark.stop()\n        return\n        \n    print(f\"üìÇ T√¨m th·∫•y {len(input_files)} ·∫£nh ƒë·∫ßu v√†o t·ª´ Module 1.\")\n    \n    # T·∫°o DataFrame\n    data = []\n    for f in input_files:\n        img_id = os.path.basename(os.path.dirname(f))\n        data.append((img_id, f, OUTPUT_DIR))\n    \n    df = spark.createDataFrame(data, [\"img_id\", \"img_path\", \"output_dir\"])\n    \n    # 2. Ch·∫°y tr√™n Spark\n    df_part = df.repartition(1) \n    print(\"üîÑ ƒêang th·ª±c hi·ªán Inversion (Qu√° tr√¨nh n√†y t·ªën GPU)...\")\n    \n    results_rdd = df_part.rdd.mapPartitions(inversion_worker)\n    final_results = results_rdd.collect()\n    \n    # 3. Hi·ªÉn th·ªã k·∫øt qu·∫£\n    print(\"\\n‚úÖ K·∫æT QU·∫¢ MODULE 2:\")\n    print(f\"{'IMAGE ID':<15} | {'STATUS':<15} | {'TIME (s)':<10} | {'MSE (Loss)'}\")\n    print(\"-\" * 65)\n    \n    success_count = 0\n    stats_data = []\n    \n    for img_id, status, p_time, mse in final_results:\n        print(f\"{img_id[:15]:<15} | {status[:15]:<15} | {p_time:<10} | {mse}\")\n        if status == \"SUCCESS\":\n            success_count += 1\n            stats_data.append({\"Image ID\": img_id, \"Time\": p_time, \"MSE\": mse})\n            \n    if stats_data:\n        pd.DataFrame(stats_data).to_csv(os.path.join(OUTPUT_DIR, \"module2_stats.csv\"), index=False)\n        \n    print(\"-\" * 65)\n    print(f\"üéâ Ho√†n th√†nh: {success_count}/{len(final_results)} ·∫£nh.\")\n    \n    spark.stop()\n    clean_memory()\n\nif __name__ == \"__main__\":\n    main_module2()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T03:37:07.763299Z","iopub.execute_input":"2025-12-25T03:37:07.763550Z","iopub.status.idle":"2025-12-25T03:39:48.285532Z","shell.execute_reply.started":"2025-12-25T03:37:07.763520Z","shell.execute_reply":"2025-12-25T03:39:48.284947Z"}},"outputs":[{"name":"stdout","text":"üöÄ Kh·ªüi ch·∫°y Spark Module 2 (Inversion) - Output: /kaggle/working/Depth-Aware-Editing/spark_output/module2\nüìÇ T√¨m th·∫•y 50 ·∫£nh ƒë·∫ßu v√†o t·ª´ Module 1.\nüîÑ ƒêang th·ª±c hi·ªán Inversion (Qu√° tr√¨nh n√†y t·ªën GPU)...\n","output_type":"stream"},{"name":"stderr","text":"üõ†Ô∏è ƒêang √°p d·ª•ng b·∫£n v√°: torch._functorch.eager_transforms.grad_and_value1) / 1]\n/usr/local/lib/python3.12/dist-packages/diffusers/models/dual_transformer_2d.py:20: FutureWarning: `DualTransformer2DModel` is deprecated and will be removed in version 0.29. Importing `DualTransformer2DModel` from `diffusers.models.dual_transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.dual_transformer_2d import DualTransformer2DModel`, instead.\n  deprecate(\"DualTransformer2DModel\", \"0.29\", deprecation_message)\n/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nFetching 13 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:14<00:00,  1.13s/it]\nLoading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:07<00:00,  1.27s/it]\nYou have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.14it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00, 10.29it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00, 10.25it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00, 10.26it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 20/21 [00:01<00:00, 10.19it/s]‚úÖ Inversion ho√†n t·∫•t.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00, 10.22it/s]\nüîÑ ƒêang Invert (20 steps)...\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 20/21 [00:01<00:00, 10.18it/s]‚úÖ Inversion ho√†n t·∫•t.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00, 10.18it/s]\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00, 10.16it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 20/21 [00:01<00:00, 10.13it/s]‚úÖ Inversion ho√†n t·∫•t.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00, 10.14it/s]\nüîÑ ƒêang Invert (20 steps)...\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 20/21 [00:01<00:00, 10.13it/s]‚úÖ Inversion ho√†n t·∫•t.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00, 10.11it/s]\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00, 10.05it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00, 10.08it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00, 10.07it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00, 10.03it/s]                     (0 + 1) / 1]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00, 10.00it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.92it/s]‚úÖ Inversion ho√†n t·∫•t.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.98it/s]\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.97it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.95it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.92it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.90it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.88it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.91it/s]‚úÖ Inversion ho√†n t·∫•t.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.89it/s]\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.87it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.89it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.89it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.88it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.88it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.85it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.87it/s]‚úÖ Inversion ho√†n t·∫•t.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.86it/s]\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.89it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.80it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.87it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.84it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.78it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.76it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.81it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.80it/s]                     (0 + 1) / 1]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.77it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.79it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.76it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.77it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.78it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.74it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.72it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.75it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.79it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.75it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.74it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.75it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.77it/s]\n‚úÖ Inversion ho√†n t·∫•t.\nüîÑ ƒêang Invert (20 steps)...\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:02<00:00,  9.78it/s]\n‚úÖ Inversion ho√†n t·∫•t.\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ K·∫æT QU·∫¢ MODULE 2:\nIMAGE ID        | STATUS          | TIME (s)   | MSE (Loss)\n-----------------------------------------------------------------\n005660          | SUCCESS         | 3.41       | 0.002601000014692545\n003141          | SUCCESS         | 2.49       | 0.00446499977260828\n004566          | SUCCESS         | 2.5        | 0.0048779998905956745\n004710          | SUCCESS         | 2.5        | 0.00395999988541007\n000419          | SUCCESS         | 2.5        | 0.002704000100493431\n004656          | SUCCESS         | 2.5        | 0.00660800002515316\n007284          | SUCCESS         | 2.51       | 0.001585999969393015\n000603          | SUCCESS         | 2.52       | 0.0030869999900460243\n003065          | SUCCESS         | 2.52       | 0.0021230001002550125\n007141          | SUCCESS         | 2.54       | 0.002633000025525689\n001669          | SUCCESS         | 2.53       | 0.006269000004976988\n007355          | SUCCESS         | 2.53       | 0.0030759999062865973\n004278          | SUCCESS         | 2.54       | 0.0022479998879134655\n001593          | SUCCESS         | 2.55       | 0.00445799995213747\n005847          | SUCCESS         | 2.55       | 0.002858000108972192\n005000          | SUCCESS         | 2.56       | 0.0034179999493062496\n004178          | SUCCESS         | 2.56       | 0.0071680000983178616\n001183          | SUCCESS         | 2.57       | 0.005462000146508217\n002312          | SUCCESS         | 2.57       | 0.0023819999769330025\n002084          | SUCCESS         | 2.58       | 0.003461000043898821\n002594          | SUCCESS         | 2.58       | 0.0020910000894218683\n003244          | SUCCESS         | 2.59       | 0.0023690001107752323\n003741          | SUCCESS         | 2.58       | 0.0019600000232458115\n004772          | SUCCESS         | 2.58       | 0.0037189999129623175\n003053          | SUCCESS         | 2.58       | 0.009595000185072422\n006386          | SUCCESS         | 2.58       | 0.002816000021994114\n003381          | SUCCESS         | 2.59       | 0.004346999805420637\n005084          | SUCCESS         | 2.59       | 0.0021230001002550125\n002584          | SUCCESS         | 2.58       | 0.0018550000386312604\n004924          | SUCCESS         | 2.6        | 0.004282000008970499\n005328          | SUCCESS         | 2.58       | 0.002580000087618828\n006796          | SUCCESS         | 2.6        | 0.0022730000782757998\n004725          | SUCCESS         | 2.61       | 0.00381199992261827\n007203          | SUCCESS         | 2.61       | 0.002435999922454357\n000313          | SUCCESS         | 2.61       | 0.008454999886453152\n005647          | SUCCESS         | 2.61       | 0.004693999886512756\n005398          | SUCCESS         | 2.62       | 0.008681000210344791\n000920          | SUCCESS         | 2.62       | 0.005688999779522419\n003380          | SUCCESS         | 2.62       | 0.005229000002145767\n000585          | SUCCESS         | 2.62       | 0.004174999892711639\n004035          | SUCCESS         | 2.61       | 0.006574000231921673\n002712          | SUCCESS         | 2.62       | 0.0042489999905228615\n006833          | SUCCESS         | 2.63       | 0.006213999819010496\n005092          | SUCCESS         | 2.62       | 0.0021609999239444733\n005504          | SUCCESS         | 2.61       | 0.004093999974429607\n007456          | SUCCESS         | 2.62       | 0.0028210000600665808\n002011          | SUCCESS         | 2.62       | 0.002520000096410513\n001840          | SUCCESS         | 2.62       | 0.0046190000139176846\n007190          | SUCCESS         | 2.62       | 0.0019330000504851341\n001703          | SUCCESS         | 2.61       | 0.006645000074058771\n-----------------------------------------------------------------\nüéâ Ho√†n th√†nh: 50/50 ·∫£nh.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# @title 8Ô∏è‚É£ RUN MODULE 3 (SPARK - FEATURE EXTRACTION): Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng h√†ng lo·∫°t\nimport os\nimport sys\nimport gc\nimport glob\nimport time\nimport shutil\nimport types\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom unittest.mock import MagicMock\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, FloatType\n\n# ================= C·∫§U H√åNH MODULE 3 =================\n# Input l·∫•y t·ª´ output c·ªßa Module 2\nINPUT_DIR_M2 = \"/kaggle/working/Depth-Aware-Editing/spark_output/module2\"\nOUTPUT_DIR_M3 = \"/kaggle/working/Depth-Aware-Editing/spark_output/module3\"\nSPARK_MASTER = \"local[1]\"\n\n# ================= üöë EMERGENCY PATCH: TORCH.MTIA =================\nif not hasattr(torch, 'mtia'):\n    torch.mtia = types.ModuleType('mtia')\nif not hasattr(torch.mtia, '_set_stream_by_id'):\n    torch.mtia._set_stream_by_id = lambda *args, **kwargs: None\nif not hasattr(torch.mtia, 'set_stream'):\n    torch.mtia.set_stream = lambda *args, **kwargs: None\nif not hasattr(torch.mtia, 'synchronize'):\n    torch.mtia.synchronize = lambda *args, **kwargs: None\nif not hasattr(torch.mtia, 'device'):\n    class FakeDevice: pass\n    torch.mtia.device = FakeDevice\n\n# ================= üõ°Ô∏è SYSTEM PATCH V4 (DIFFUSERS) =================\nsys.modules[\"torch.onnx\"] = MagicMock()\nif \"diffusers.hooks\" not in sys.modules:\n    hooks_pkg = types.ModuleType(\"diffusers.hooks\")\n    hooks_pkg.__path__ = [] \n    sys.modules[\"diffusers.hooks\"] = hooks_pkg\n\nif \"diffusers.hooks.group_offloading\" not in sys.modules:\n    offload_mod = types.ModuleType(\"diffusers.hooks.group_offloading\")\n    sys.modules[\"diffusers.hooks.group_offloading\"] = offload_mod\nelse:\n    offload_mod = sys.modules[\"diffusers.hooks.group_offloading\"]\n\nsys.modules[\"diffusers.hooks\"].group_offloading = offload_mod\ndef _fake_get_group_onload_device(*args, **kwargs): return torch.device(\"cpu\")\ndef _fake_is_group_offload_enabled(*args, **kwargs): return False\noffload_mod._get_group_onload_device = _fake_get_group_onload_device\noffload_mod._is_group_offload_enabled = _fake_is_group_offload_enabled\n\ntry:\n    import diffusers.models.model_loading_utils\n    def _bypass_check(*args, **kwargs): return None\n    diffusers.models.model_loading_utils.check_support_param_buffer_assignment = _bypass_check\nexcept ImportError: pass\n\n# ================= CLASS: FEATURE EXTRACTOR =================\nclass FeatureExtractor:\n    def __init__(self, unet):\n        self.unet = unet\n        self.features = {}\n        self.hooks = []\n        \n        # Hook v√†o c√°c t·∫ßng Up-block (Decoder)\n        for i, block in enumerate(unet.up_blocks):\n            layer_name = f\"up_block_{i}\"\n            hook = block.register_forward_hook(self._get_hook_fn(layer_name))\n            self.hooks.append(hook)\n            \n        if hasattr(unet, \"mid_block\") and unet.mid_block is not None:\n             hook = unet.mid_block.register_forward_hook(self._get_hook_fn(\"mid_block\"))\n             self.hooks.append(hook)\n\n    def _get_hook_fn(self, layer_name):\n        def hook(module, input, output):\n            if isinstance(output, tuple): feature_map = output[0]\n            else: feature_map = output\n            self.features[layer_name] = feature_map.detach().cpu()\n        return hook\n\n    def remove_hooks(self):\n        for hook in self.hooks: hook.remove()\n        self.hooks = []\n\n# ================= LOGIC WORKER (SPARK) =================\ndef feature_worker(iterator):\n    import torch\n    import os\n    import numpy as np\n    from diffusers import StableDiffusionPipeline, DDIMScheduler\n\n    # Load Model (1 l·∫ßn cho c·∫£ partition)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n    \n    try:\n        scheduler = DDIMScheduler.from_pretrained(MODEL_ID, subfolder=\"scheduler\")\n        pipe = StableDiffusionPipeline.from_pretrained(\n            MODEL_ID, scheduler=scheduler, torch_dtype=torch.float32, # Feature extraction c·∫ßn ƒë·ªô ch√≠nh x√°c cao\n            low_cpu_mem_usage=False, device_map=None, use_safetensors=True\n        ).to(device)\n        pipe.set_progress_bar_config(disable=True)\n    except Exception as e:\n        yield (\"INIT_ERROR\", str(e), \"\", 0, 0, 0, 0)\n        return\n\n    # Loop x·ª≠ l√Ω t·ª´ng ·∫£nh\n    for row in iterator:\n        img_id = row.img_id\n        latent_path = row.latent_path\n        \n        try:\n            # 1. Load Latent\n            # File t·ª´ Module 2 l√† 'inverted_latents.pt'\n            latents = torch.load(latent_path).to(device, dtype=torch.float32)\n\n            # 2. Setup Extractor\n            extractor = FeatureExtractor(pipe.unet)\n\n            # 3. Forward Pass (Ch·∫°y qua UNet ƒë·ªÉ k√≠ch ho·∫°t features)\n            t = torch.tensor([0]).to(device) # timestep 0\n            with torch.no_grad():\n                prompt_embeds, _ = pipe.encode_prompt(\n                    prompt=\"\", device=device, num_images_per_prompt=1, do_classifier_free_guidance=False\n                )\n                prompt_embeds = prompt_embeds.to(dtype=torch.float32)\n                pipe.unet(latents, t, encoder_hidden_states=prompt_embeds)\n            \n            # 4. L∆∞u Features & T√≠nh Stats\n            save_dir = os.path.join(OUTPUT_DIR_M3, img_id)\n            os.makedirs(save_dir, exist_ok=True)\n            \n            # L∆∞u stats c·ªßa layer quan tr·ªçng nh·∫•t: 'up_block_2' (th∆∞·ªùng ch·ª©a semantic t·ªët nh·∫•t)\n            target_layer = \"up_block_2\"\n            stats_tuple = (0, 0, 0, 0)\n            \n            for name, tensor in extractor.features.items():\n                # L∆∞u file .pt\n                torch.save(tensor, os.path.join(save_dir, f\"{name}.pt\"))\n                \n                # T√≠nh stats n·∫øu l√† layer m·ª•c ti√™u\n                if name == target_layer:\n                    arr = tensor.numpy()\n                    stats_tuple = (\n                        float(np.mean(arr)),\n                        float(np.std(arr)),\n                        float(np.min(arr)),\n                        float(np.max(arr))\n                    )\n            \n            # V·∫Ω heatmap ƒë·∫°i di·ªán (l∆∞u ·∫£nh png)\n            if target_layer in extractor.features:\n                feat = extractor.features[target_layer]\n                heatmap = feat[0].mean(dim=0).float().numpy()\n                plt.figure(figsize=(4, 4))\n                plt.imshow(heatmap, cmap='magma')\n                plt.axis('off')\n                plt.tight_layout()\n                plt.savefig(os.path.join(save_dir, \"heatmap_viz.png\"))\n                plt.close()\n\n            extractor.remove_hooks()\n            \n            yield (img_id, \"SUCCESS\", target_layer, *stats_tuple)\n            \n        except Exception as e:\n            yield (img_id, f\"ERROR: {str(e)}\", \"\", 0, 0, 0, 0)\n\n# ================= SPARK DRIVER =================\ndef main_module3():\n    # D·ªçn d·∫πp\n    if 'spark' in globals(): globals()['spark'].stop()\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    print(f\"üöÄ Kh·ªüi ch·∫°y Spark Module 3 (Feature Extraction)\")\n    print(f\"üìÇ Input: {INPUT_DIR_M2}\")\n    print(f\"üìÇ Output: {OUTPUT_DIR_M3}\")\n    \n    os.makedirs(OUTPUT_DIR_M3, exist_ok=True)\n    \n    spark = SparkSession.builder \\\n        .appName(\"Module3_Features\") \\\n        .config(\"spark.driver.memory\", \"4g\") \\\n        .master(SPARK_MASTER) \\\n        .getOrCreate()\n\n    # 1. Qu√©t d·ªØ li·ªáu t·ª´ Module 2\n    # C·∫•u tr√∫c: module2/{img_id}/inverted_latents.pt\n    search_pattern = os.path.join(INPUT_DIR_M2, \"*\", \"inverted_latents.pt\")\n    latent_files = glob.glob(search_pattern)\n    \n    if not latent_files:\n        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file 'inverted_latents.pt' n√†o trong {INPUT_DIR_M2}\")\n        spark.stop()\n        return\n    \n    print(f\"‚úÖ T√¨m th·∫•y {len(latent_files)} b·ªô Latent ƒë·ªÉ x·ª≠ l√Ω.\")\n    \n    # 2. T·∫°o DataFrame\n    data = []\n    for f in latent_files:\n        img_id = os.path.basename(os.path.dirname(f))\n        data.append((img_id, f))\n        \n    df = spark.createDataFrame(data, [\"img_id\", \"latent_path\"])\n    df_part = df.repartition(1)\n    \n    # 3. Ch·∫°y Worker\n    print(\"üîÑ ƒêang tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng (UNet Forward Pass)...\")\n    results_rdd = df_part.rdd.mapPartitions(feature_worker)\n    \n    # ƒê·ªãnh nghƒ©a Schema cho k·∫øt qu·∫£\n    schema = StructType([\n        StructField(\"Image_ID\", StringType(), True),\n        StructField(\"Status\", StringType(), True),\n        StructField(\"Layer\", StringType(), True),\n        StructField(\"Mean\", FloatType(), True),\n        StructField(\"Std\", FloatType(), True),\n        StructField(\"Min\", FloatType(), True),\n        StructField(\"Max\", FloatType(), True)\n    ])\n    \n    # Collect v·ªÅ DataFrame ƒë·ªÉ hi·ªÉn th·ªã ƒë·∫πp\n    try:\n        final_df = spark.createDataFrame(results_rdd, schema=schema)\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"üìä B·∫¢NG TH·ªêNG K√ä FEATURE MAPS (L·ªõp: up_block_2)\")\n        print(\"=\"*80)\n        final_df.show(truncate=False)\n        \n        # L∆∞u b·∫£ng th·ªëng k√™\n        final_df.toPandas().to_csv(os.path.join(OUTPUT_DIR_M3, \"feature_stats.csv\"), index=False)\n        \n        # ƒê·∫øm th√†nh c√¥ng\n        success_count = final_df.filter(\"Status = 'SUCCESS'\").count()\n        print(f\"üéâ Ho√†n th√†nh: {success_count}/{len(latent_files)} ·∫£nh.\")\n        \n    except Exception as e:\n        print(f\"‚ùå L·ªói khi hi·ªÉn th·ªã b·∫£ng: {e}\")\n        # In raw n·∫øu l·ªói DataFrame\n        print(results_rdd.collect())\n\n    spark.stop()\n\nif __name__ == \"__main__\":\n    main_module3()","metadata":{"execution":{"iopub.status.busy":"2025-12-25T03:46:00.871433Z","iopub.execute_input":"2025-12-25T03:46:00.872105Z","iopub.status.idle":"2025-12-25T03:47:47.547780Z","shell.execute_reply.started":"2025-12-25T03:46:00.872075Z","shell.execute_reply":"2025-12-25T03:47:47.547148Z"},"trusted":true},"outputs":[{"name":"stdout","text":"üöÄ Kh·ªüi ch·∫°y Spark Module 3 (Feature Extraction)\nüìÇ Input: /kaggle/working/Depth-Aware-Editing/spark_output/module2\nüìÇ Output: /kaggle/working/Depth-Aware-Editing/spark_output/module3\n‚úÖ T√¨m th·∫•y 50 b·ªô Latent ƒë·ªÉ x·ª≠ l√Ω.\nüîÑ ƒêang tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng (UNet Forward Pass)...\n","output_type":"stream"},{"name":"stderr","text":"[Stage 0:>                                                          (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nüìä B·∫¢NG TH·ªêNG K√ä FEATURE MAPS (L·ªõp: up_block_2)\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nFetching 14 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:04<00:00,  3.14it/s]\nLoading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:09<00:00,  1.43s/it]\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+--------+-------+----------+----------+---------+----------+---------+\n|Image_ID|Status |Layer     |Mean      |Std      |Min       |Max      |\n+--------+-------+----------+----------+---------+----------+---------+\n|005660  |SUCCESS|up_block_2|0.27110583|2.880915 |-19.529297|27.833855|\n|003141  |SUCCESS|up_block_2|0.2866447 |2.8925755|-19.278198|22.246082|\n|004566  |SUCCESS|up_block_2|0.28827026|2.939748 |-18.113901|19.707815|\n|004710  |SUCCESS|up_block_2|0.2710388 |2.927826 |-18.65343 |17.784754|\n|000419  |SUCCESS|up_block_2|0.265091  |2.896575 |-20.215073|22.331396|\n|004656  |SUCCESS|up_block_2|0.26360095|2.9126134|-17.390797|18.613302|\n|007284  |SUCCESS|up_block_2|0.28630087|2.8744402|-17.68321 |26.28304 |\n|000603  |SUCCESS|up_block_2|0.2912592 |2.93584  |-18.605118|19.507841|\n|003065  |SUCCESS|up_block_2|0.2803051 |2.8965063|-17.611792|21.625946|\n|007141  |SUCCESS|up_block_2|0.24045257|2.885831 |-17.944359|17.95273 |\n|001669  |SUCCESS|up_block_2|0.26341736|2.929938 |-17.766838|19.84532 |\n|007355  |SUCCESS|up_block_2|0.27654725|2.9102063|-18.10694 |17.551329|\n|004278  |SUCCESS|up_block_2|0.26243666|2.7848163|-18.79944 |21.215929|\n|001593  |SUCCESS|up_block_2|0.29096478|2.9160886|-20.20712 |22.183601|\n|005847  |SUCCESS|up_block_2|0.3003801 |2.9095638|-19.002638|20.67888 |\n|005000  |SUCCESS|up_block_2|0.29115647|2.920707 |-17.723236|22.220015|\n|004178  |SUCCESS|up_block_2|0.26675296|2.9342592|-19.081402|18.9066  |\n|001183  |SUCCESS|up_block_2|0.25268304|2.8918605|-18.183401|25.26886 |\n|002312  |SUCCESS|up_block_2|0.30630392|2.8888073|-19.483757|21.393148|\n|002084  |SUCCESS|up_block_2|0.2695616 |2.9025838|-19.96795 |18.05022 |\n+--------+-------+----------+----------+---------+----------+---------+\nonly showing top 20 rows\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nLoading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:09<00:00,  1.36s/it]\n/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nLoading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:09<00:00,  1.37s/it] 1]\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"üéâ Ho√†n th√†nh: 50/50 ·∫£nh.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ==============================================================================\n# CELL \"C·∫§P C·ª®U\": GI·∫¢I PH√ìNG RAM CPU V√Ä GPU TRI·ªÜT ƒê·ªÇ\n# ==============================================================================\nimport torch\nimport gc\nimport ctypes\nimport sys\n\nprint(\"üßπ ƒêang ti·∫øn h√†nh t·ªïng v·ªá sinh b·ªô nh·ªõ (CPU & GPU)...\")\n\n# 1. X√≥a c√°c bi·∫øn l·ªõn trong Python\n# Danh s√°ch c√°c t√™n bi·∫øn th∆∞·ªùng d√πng trong notebook n√†y\ntargets = ['model', 'diff_handles', 'activations', 'loaded_feats', \n           'init_noise', 'depth_tensor', 'mpi_masks', 'results', \n           'sd', 'state_dict', 'checkpoint']\n\nfor t in targets:\n    if t in globals():\n        del globals()[t]\n        print(f\"   - ƒê√£ x√≥a bi·∫øn Python: {t}\")\n\n# 2. √âp Garbage Collector ch·∫°y\ngc.collect()\n\n# 3. QUAN TR·ªåNG: √âp Linux thu h·ªìi b·ªô nh·ªõ RAM (Malloc Trim)\n# L·ªánh n√†y gi√∫p RAM gi·∫£m ngay l·∫≠p t·ª©c tr√™n Kaggle/Colab\ntry:\n    libc = ctypes.CDLL(\"libc.so.6\")\n    libc.malloc_trim(0)\n    print(\"   - ƒê√£ ch·∫°y l·ªánh h·ªá th·ªëng: libc.malloc_trim(0)\")\nexcept Exception as e:\n    print(f\"   - Kh√¥ng g·ªçi ƒë∆∞·ª£c libc (Kh√¥ng sao): {e}\")\n\n# 4. X·∫£ Cache GPU\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n    print(\"   - ƒê√£ x·∫£ CUDA Cache.\")\n\nprint(\"-\" * 30)\nprint(\"‚úÖ HO√ÄN T·∫§T! H√ÉY KI·ªÇM TRA L·∫†I THANH RAM ·ªû G√ìC TR√äN.\")\nprint(\"üëâ Sau ƒë√≥ ch·∫°y l·∫°i Cell Load Model b√™n d∆∞·ªõi.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# B∆Ø·ªöC 2: LOAD MODEL SD 1.5 INPAINTING (B·∫¢N V√Å V11 - FIX INPAINTING CONFIG)\n# ==============================================================================\nimport torch\nimport os\nimport sys\nimport requests\nimport glob\nimport gc\nimport ctypes\nimport types\nimport yaml\nfrom unittest.mock import MagicMock\n\nprint(\"üöë ƒêANG K√çCH HO·∫†T H·ªÜ TH·ªêNG V√Å L·ªñI V11 (CHANNEL MISMATCH FIX)...\")\n\n# --- 1. S·ª¨A FILE CONFIG T·ª™ 4 K√äNH -> 9 K√äNH (L·ªñI C·ª¶A B·∫†N) ---\n# T·∫£i config chu·∫©n tr∆∞·ªõc\nconfig_path = \"cldm_v15_fixed.yaml\"\nif not os.path.exists(config_path):\n    url = \"https://raw.githubusercontent.com/lllyasviel/ControlNet/main/models/cldm_v15.yaml\"\n    try:\n        with open(config_path, \"wb\") as f: f.write(requests.get(url).content)\n    except: pass\n\n# ƒê·ªçc v√† s·ª≠a config\ntry:\n    with open(config_path, 'r') as f:\n        config_data = yaml.safe_load(f)\n    \n    # Ki·ªÉm tra v√† s·ª≠a in_channels\n    current_channels = config_data['model']['params']['unet_config']['params']['in_channels']\n    print(f\"   ‚ÑπÔ∏è Config g·ªëc c√≥ {current_channels} k√™nh ƒë·∫ßu v√†o.\")\n    \n    if current_channels != 9:\n        print(\"   üîß ƒêang chuy·ªÉn ƒë·ªïi Config sang ch·∫ø ƒë·ªô Inpainting (9 k√™nh)...\")\n        config_data['model']['params']['unet_config']['params']['in_channels'] = 9\n        \n        # L∆∞u ra file m·ªõi\n        new_config_path = \"cldm_v15_inpainting_patched.yaml\"\n        with open(new_config_path, 'w') as f:\n            yaml.dump(config_data, f)\n        config_path = new_config_path # Tr·ªè sang file m·ªõi\n        print(f\"   ‚úÖ ƒê√£ t·∫°o config m·ªõi: {config_path}\")\n    else:\n        print(\"   ‚úÖ Config ƒë√£ chu·∫©n 9 k√™nh.\")\n\nexcept Exception as e:\n    print(f\"   ‚ö†Ô∏è L·ªói khi s·ª≠a config: {e}\")\n\n# --- 2. FIX C√ÅC L·ªñI KH√ÅC (GI·ªÆ NGUY√äN T·ª™ V10) ---\n# Fix missing anydoor config\nrequired_config_path = \"/kaggle/working/configs/anydoor.yaml\"\nif not os.path.exists(required_config_path):\n    os.makedirs(os.path.dirname(required_config_path), exist_ok=True)\n    dummy_content = \"\"\"\nmodel:\n  target: ldm.models.diffusion.ddpm.LatentDiffusion\n  params:\n    linear_start: 0.00085\n    linear_end: 0.0120\n    num_timesteps_cond: 1\n    log_every_t: 200\n    timesteps: 1000\n    first_stage_key: \"jpg\"\n    cond_stage_key: \"txt\"\n    image_size: 64\n    channels: 4\n    cond_stage_trainable: false\n    conditioning_key: crossattn\n    monitor: val/loss_simple_ema\n    scale_factor: 0.18215\n    use_ema: False\n    cond_stage_config:\n      target: ldm.modules.encoders.modules.FrozenOpenCLIPEmbedder\n      params:\n        freeze: True\n        layer: \"penultimate\"\n      weight: \"weights/dinov2_vitg14_pretrain.pth\"\n\"\"\"\n    with open(required_config_path, \"w\") as f: f.write(dummy_content)\n\n# Patch ONNX & FSDP\nfake_onnx = types.ModuleType(\"torch.onnx\")\nfake_onnx.__path__ = []\nfake_onnx.symbolic_helper = MagicMock()\nfake_onnx.utils = MagicMock()\nfake_onnx.symbolic_opset11 = MagicMock()\nfake_onnx.symbolic_opset9 = MagicMock()\nfake_onnx.symbolic_opset10 = MagicMock()\nfake_onnx.register_custom_op_symbolic = MagicMock(return_value=None)\nfake_onnx.unregister_custom_op_symbolic = MagicMock(return_value=None)\nfake_onnx.select_model_mode_for_export = MagicMock(return_value=None)\n\nsys.modules[\"torch.onnx\"] = fake_onnx\nsys.modules[\"torch.onnx.symbolic_helper\"] = fake_onnx.symbolic_helper\nsys.modules[\"torch.onnx.utils\"] = fake_onnx.utils\nsys.modules[\"torch.onnx.symbolic_opset11\"] = fake_onnx.symbolic_opset11\nfake_onnx.symbolic_helper.parse_args = MagicMock(return_value=lambda x: x)\nfake_onnx.symbolic_helper._parse_arg = MagicMock(return_value=lambda x: x)\n\nclass MockClass:\n    def __init__(self, *args, **kwargs): pass\n\nfake_fsdp = types.ModuleType(\"torch.distributed.fsdp\")\nfake_fsdp.__path__ = []\nfake_fsdp.CPUOffload = MockClass\nfake_fsdp.FullyShardedDataParallel = MockClass\nfake_fsdp.MixedPrecision = MockClass\nfake_fsdp.BackwardPrefetch = MockClass\nfake_fsdp.ShardingStrategy = MockClass\nfake_fsdp.StateDictType = MockClass\nfake_fsdp.FullStateDictConfig = MockClass\n\nsys.modules[\"torch.distributed.fsdp\"] = fake_fsdp\nsys.modules[\"torch.distributed.fsdp.fully_sharded_data_parallel\"] = fake_fsdp\nsys.modules[\"torch.distributed.fsdp.sharded_grad_scaler\"] = MagicMock()\nsys.modules[\"torch.distributed.fsdp.wrap\"] = MagicMock()\n\nfake_algo = types.ModuleType(\"torch.distributed.algorithms\")\nsys.modules[\"torch.distributed.algorithms\"] = fake_algo\nsys.modules[\"torch.distributed.algorithms.model_averaging\"] = MagicMock()\nsys.modules[\"torch.distributed.algorithms.model_averaging.averagers\"] = MagicMock()\n\nprint(\"   ‚úÖ [2/2] ƒê√£ v√° Environment & Config.\")\nprint(\"üöÄ TI·∫æP T·ª§C LOAD MODEL...\")\n# -----------------------------------------------------------\n\n# Setup path\nsys.path.append(os.getcwd())\nif os.path.exists(\"/kaggle/working/Depth-Aware-Editing\"):\n    sys.path.append(\"/kaggle/working/Depth-Aware-Editing\")\n\ntry:\n    from cldm.model import create_model, load_state_dict\n    from cldm.ddim_hacked_mpi_featguidance import DDIMSampler\nexcept ImportError:\n    if os.path.exists(\"/kaggle/temp/Depth-Aware-Editing\"):\n        os.chdir(\"/kaggle/temp/Depth-Aware-Editing\")\n        sys.path.append(\"/kaggle/temp/Depth-Aware-Editing\")\n        from cldm.model import create_model, load_state_dict\n        from cldm.ddim_hacked_mpi_featguidance import DDIMSampler\n    else:\n        print(\"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c source code!\")\n\n# Ch·ªçn GPU 0\ntarget_device = \"cuda:0\"\nckpt_path = '/kaggle/working/Depth-Aware-Editing/weights/sd-v1-5-inpainting.ckpt'\n\n# S·ª≠ d·ª•ng config_path ƒë√£ ƒë∆∞·ª£c v√° ·ªü tr√™n\nprint(f\"‚ÑπÔ∏è S·ª≠ d·ª•ng Config: {config_path}\")\n\ntry:\n    print(\"üì¶ T·∫°o khung Model (CPU)...\")\n    import warnings\n    warnings.filterwarnings(\"ignore\")\n    \n    model = create_model(config_path).cpu()\n    \n    print(\"üìÇ ƒêang ƒë·ªçc checkpoint...\")\n    if not os.path.exists(ckpt_path):\n        found = glob.glob(\"/kaggle/working/**/sd-v1-5-inpainting.ckpt\", recursive=True)\n        if found: ckpt_path = found[0]\n        else: \n            print(\"‚¨áÔ∏è ƒêang t·∫£i weights d·ª± ph√≤ng...\")\n            os.system(f\"wget -q -O {ckpt_path} https://huggingface.co/runwayml/stable-diffusion-inpainting/resolve/main/sd-v1-5-inpainting.ckpt\")\n            \n    if not os.path.exists(ckpt_path): raise FileNotFoundError(\"Kh√¥ng t√¨m th·∫•y file checkpoint!\")\n\n    # Load Weights\n    sd = load_state_dict(ckpt_path, location='cpu')\n    if 'state_dict' in sd: sd = sd['state_dict']\n    \n    print(\"‚è≥ ƒêang n·∫°p weights v√†o model (Strict=False)...\")\n    m, u = model.load_state_dict(sd, strict=False)\n    print(f\"   -> Model loaded. Missing keys: {len(m)}, Unexpected keys: {len(u)}\")\n    \n    # Clean RAM\n    del sd\n    gc.collect()\n    try: ctypes.CDLL(\"libc.so.6\").malloc_trim(0) \n    except: pass\n\n    # Move to GPU (FIX: D√πng FP32 thay v√¨ FP16)\n    print(f\"üöö ƒêang ƒë·∫©y model sang {target_device} (FP32)...\")\n    model = model.float().to(target_device)  # FIX: float() thay v√¨ half()\n    \n    diff_handles = DDIMSampler(model)\n    device = torch.device(target_device)\n    \n    print(\"=\"*60)\n    print(\"üéâ LOAD MODEL TH√ÄNH C√îNG!\")\n    print(f\"   - Checkpoint: {os.path.basename(ckpt_path)}\")\n    print(f\"   - VRAM Used: {torch.cuda.memory_allocated(device)/1e9:.2f} GB\")\n    print(\"üëâ Model ƒë√£ s·∫µn s√†ng. H√£y ch·∫°y ti·∫øp Cell Module 4.\")\n    print(\"=\"*60)\n\nexcept Exception as e:\n    print(f\"‚ùå V·∫´n c√≤n l·ªói: {e}\")\n    import traceback\n    traceback.print_exc()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title üöÄ MODULE 4: Scene Composition (Simplified + Spark)\nimport os\nimport sys\nimport gc\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport torch.nn. functional as F\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql. types import StructType, StructField, StringType\nimport matplotlib. pyplot as plt\n\nprint(\"üöÄ MODULE 4: SIMPLIFIED SCENE COMPOSITION\")\n\n# ================= CONFIG =================\nBASE_WORK_DIR = \"/kaggle/working/Depth-Aware-Editing/spark_output\"\nDIR_M1 = os.path.join(BASE_WORK_DIR, \"module1\")\nDIR_M2 = os.path.join(BASE_WORK_DIR, \"module2\")\nDIR_M3 = os.path.join(BASE_WORK_DIR, \"module3\")\nOUTPUT_DIR_M4 = os.path.join(BASE_WORK_DIR, \"module4\")\nPROMPT = \"a high quality photo of a living room\"\nSPARK_MASTER = \"local[1]\"\n\ndef clean_memory():\n    for var in ['pipe', 'model']: \n        if var in globals(): del globals()[var]\n    gc.collect()\n    if torch.cuda.is_available(): torch.cuda.empty_cache()\n\nclean_memory()\n\n# ================= WORKER =================\ndef generation_worker(iterator):\n    import os, sys, torch, numpy as np\n    from PIL import Image\n    from scipy.ndimage import gaussian_filter\n    \n    sys.path.insert(0, \"/kaggle/working/Depth-Aware-Editing\")\n    \n    try:\n        from diffusers import StableDiffusionImg2ImgPipeline\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n            \"runwayml/stable-diffusion-v1-5\",\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n            safety_checker=None\n        ).to(device)\n        pipe.set_progress_bar_config(disable=True)\n    except Exception as e:\n        yield (\"INIT_ERROR\", str(e), \"\")\n        return\n    \n    for row in iterator:\n        img_id = row. img_id\n        try:\n            path_m1 = os.path.join(DIR_M1, img_id)\n            \n            # Load image & mask\n            input_img = Image.open(os.path.join(path_m1, \"input.png\")).convert(\"RGB\").resize((512, 512))\n            mask_pil = Image.open(os. path.join(path_m1, \"mask.png\")).convert(\"L\").resize((512, 512))\n            mask_np = np.array(mask_pil) / 255.0\n            \n            # Blur masked region\n            input_np = np.array(input_img)\n            blurred = gaussian_filter(input_np, sigma=[5, 5, 0])\n            mask_3ch = np.stack([mask_np]*3, axis=-1)\n            blended = input_np * (1 - mask_3ch) + blurred * mask_3ch\n            blended_img = Image.fromarray(blended.astype(np.uint8))\n            \n            # Generate\n            with torch.no_grad():\n                output = pipe(\n                    prompt=PROMPT,\n                    image=blended_img,\n                    strength=0.75,\n                    guidance_scale=7.5,\n                    num_inference_steps=50\n                ).images[0]\n            \n            # Save\n            save_dir = os.path.join(OUTPUT_DIR_M4, img_id)\n            os.makedirs(save_dir, exist_ok=True)\n            output_path = os.path.join(save_dir, \"final_result.png\")\n            output. save(output_path)\n            \n            # Save comparison\n            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n            axes[0].imshow(input_img); axes[0].set_title(\"Input\"); axes[0].axis('off')\n            axes[1].imshow(mask_pil, cmap='gray'); axes[1].set_title(\"Mask\"); axes[1].axis('off')\n            axes[2].imshow(output); axes[2].set_title(\"Output\"); axes[2].axis('off')\n            plt.tight_layout()\n            plt.savefig(os.path.join(save_dir, \"comparison.png\"), dpi=100)\n            plt.close()\n            \n            yield (img_id, \"SUCCESS\", output_path)\n            \n        except Exception as e: \n            yield (img_id, \"ERROR\", str(e)[:80])\n\n# ================= MAIN =================\ndef main():\n    if not os.path.exists(DIR_M1):\n        print(\"‚ùå Module 1 ch∆∞a ch·∫°y!\")\n        return\n    \n    img_ids = [d for d in os.listdir(DIR_M1) if os.path.isdir(os.path.join(DIR_M1, d))]\n    if not img_ids:\n        print(\"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu\")\n        return\n    \n    print(f\" Found {len(img_ids)} images\")\n    os.makedirs(OUTPUT_DIR_M4, exist_ok=True)\n    \n    # Spark\n    spark = SparkSession.builder \\\n        .appName(\"Module4\") \\\n        .config(\"spark. driver.memory\", \"4g\") \\\n        .master(SPARK_MASTER) \\\n        .getOrCreate()\n    \n    spark.sparkContext. setLogLevel(\"ERROR\")\n    \n    schema = StructType([StructField(\"img_id\", StringType(), True)])\n    df = spark.createDataFrame([(i,) for i in img_ids], schema=schema)\n    \n    print(\"\\n Processing...\")\n    results = df.rdd.mapPartitions(generation_worker).collect()\n    \n    # Results\n    print(\"\\n\" + \"=\"*80)\n    print(f\"{'IMAGE ID':<20} | {'STATUS':<15} | {'OUTPUT'}\")\n    print(\"-\"*80)\n    \n    success = 0\n    for img_id, status, note in results:\n        display_note = note[-40: ] if len(note) > 40 else note\n        if status == \"SUCCESS\":\n            success += 1\n            print(f\"{img_id:<20} | {status:<13} | ... {display_note}\")\n        else:\n            print(f\"{img_id:<20} | {status:<13} | {display_note}\")\n    \n    print(\"-\"*80)\n    print(f\"Success: {success}/{len(results)}\")\n    print(f\"Output: {OUTPUT_DIR_M4}\")\n    print(\"=\"*80)\n    \n    spark.stop()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}